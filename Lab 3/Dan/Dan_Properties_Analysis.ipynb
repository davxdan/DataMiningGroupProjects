{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Assignment\n",
    "__Authors__: Dan Davieau, Paul Panek, Olga Tanyuk, Nathan Wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Business Understanding  \n",
    "<font color=blue>Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific\n",
    "dataset and the stakeholders needs?</font>\n",
    "\n",
    "Washington, D.C. is the capital of the United States. Washington's population is approaching 700,000 people, and has been growing since 2000 following a half-century of population decline. The city is highly segregated and features a high cost of living. In 2017, the average price of a single family home in the district was $649,000. \n",
    "\n",
    "Understanding the various features and similarities between the various neighborhoods provides important insights into the housing market within the district. This analysis will serve as means for a real estate agency to develop profiles of the various wards in DC. By better understanding the housing characteristics of the different wards they can better recommend specific areas to begin there search based on the clients needs.\n",
    "\n",
    "We will measure effectivenss of different clustering algorithms to seperate various types of homes from the DC data set. For example single family homes on large plots of land should be considered as a different type of home than downtown flat in a 10 story building. The process of evaluating will be based on two criteria.\n",
    "\n",
    "1) Silhouette Coefficient and other statistical measures to determing the correct amount of clusters to use and our ability to seperate them into distinct categories.\n",
    "\n",
    "2) Manual evaluation of the observations to identify the similarity between the various clusters based on the our own ability to differentiate between different styles of homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Understanding 1\n",
    "<font color=blue>Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?</font>\n",
    "\n",
    "For this analysis we will be borrowing the \"raw_residential_data\" and \"raw_address_points.csv\" data sets from Christopher Coreeas's *\"DC Residential Properties\"* Kaggle: https://www.kaggle.com/christophercorrea/dc-residential-properties.  \n",
    "\n",
    "### raw_residential_data.csv \n",
    "The Computer Assisted Mass Appraisal - Residential data contains attribution on housing characteristics for residential properties, and was created as part of the DC Geographic Information System (DC GIS) for the D.C. Office of the Chief Technology Officer (OCTO) and participating D.C. government agencies.\n",
    "\n",
    "> __OBJECTID__= Auto-generated internal unique ID  \n",
    "__SSL__: Square, suffix, lot ID  \n",
    "__BATHRM__: Number of full bathrooms  \n",
    "__HF_BATHRM__: Number of half bathrooms (no shower or tub)  \n",
    "__HEAT__: Heating code *0 = No Data, 1 = Forced Air, 2 = Air-Oil, 3 = Wall Furnace, 4 = Electric Rad, 5 = Elec Base Brd, \n",
    "6 = Water Base Brd, 7 = Warm Cool, 8 = Ht Pump, 9 = Evp Cool, 10 = Air Exchng, 11 = Gravity Furnac, 12 = Ind Unit, 13 = Hot Water Rad*  \n",
    "__AC:__ Air conditioning (Y/N)  \n",
    "__NUM_UNITS:__ Number of units  \n",
    "__ROOMS:__ Number of rooms  \n",
    "__BEDRM:__ Number of bedrooms  \n",
    "__AYB:__ The earliest time the main portion of the building was built. It is not affected by subsequent construction.  \n",
    "__STORIES:__ Stories  \n",
    "__SALE_NUM:__ Sale number  \n",
    "__GBA:__ Gross building area in square feet  \n",
    "__STYLE:__ Style code  *0 = Default, 1 = 1 Story, 2 = 1.5 Story Unfin, 3 = 1.5 Story Fin, 4 = 2 Story, 5 = 2.5 Story Unfin, 6 = 2.5 Story Fin, 7 = 3 Story, 8 = 3.5 Story, 9 = 3.5 Story Fin, 10 = 4 Story, 11 = 4.5 Story Unfin, 12 = 4.5 Story Fin, 13 = Bi-Level, 14 = Split Level, 15 = Split Foyer, 94 = Outbuildings, 99 = Vacant*  \n",
    "__STRUCT:__ Structure code *0 = Default, 1 = Single, 2 = Multi, 4 = Town End, 5 = Town Inside, 6 = Row End, 7 = Row Inside, 8 = Semi-Detached, 13 = Vacant Land*  \n",
    "__EXTWALL:__ Exterior wall code *0 = Default, 1 = Plywood, 2 = Hardboard, 3 = Metal Siding, 4 = Vinyl Siding, 5 = Stucco, 6 = Wood Siding, 7 = Shingle, 8 = SPlaster, 9 = Rustic Log, 10 = Brick Veneer, 11 = Stone Veneer, 12 = Concrete Block, 13 = Stucco Block, 14 = Common Brick, 15 = Face Brick, 16 = Adobe, 17 = Stone, 18 = Concrete, 19 = Aluminum, 20 = Brick/Stone, 21 = Brick/Stucco, 22 = Brick/Siding, 23 = Stone/Stucco, 24 = Stone/Siding*  \n",
    "__ROOF:__ Roof type code *0 = Typical, 1 = Comp Shingle, 2 = Built Up, 3 = Shingle, 4 = Shake, 5 = Metal- Pre, 6 = Metal- Sms, 7 = Metal- Cpr, 8 = Composition Ro, 9 = Concrete Tile, 10 = Clay Tile, 11 = Slate, 12 = Concrete, 13 = Neopren, 14 = Water Proof, 15 = Wood- FS*  \n",
    "__INTWALL:__  Interior wall code *, 0 = Default, 1 = Resiliant, 2 = Carpet, 3 = Wood Floor, 4 = Ceramic Tile, 5 = Terrazo, 6 = Hardwood, 7 = Parquet, 8 = Vinyl Comp, 9 = Vinyl Sheet, 10 = Lt Concrete, 11 = Hardwood/Carp*  \n",
    "__KITCHENS:__ Number of kitchens  \n",
    "__FIREPLACES:__ Number of fireplaces  \n",
    "__LANDAREA:__ Land area of property in square feet  \n",
    "__LATITUDE:__ Lattitude of address  \n",
    "__LONGITUDE:__ Longitude of Address  \n",
    "__ASSESSMENT_NBHD:__ DC Neighborhood  \n",
    "__WARD:__ DC Wards  \n",
    "\n",
    "### raw_address_points.csv\n",
    "The raw address points data contains locations and attributes of Address points as of July 2018. This file is part of the Master Address Repository (MAR) for the D.C. Office of the Chief Technology Officer and DC Department of Consumer and Regulatory Affairs. It contains the standardized addresses in the District of Columbia which are typically placed on the buildings. \n",
    "\n",
    "-  __SSL__ There is no useful information is included in the metadata that describes this attribute except that it represents \"Square Suffix and Lot\". Upon researching http://opendata.dc.gov/datasets/3c8c90b33dbe41cd965634dda58dfd80_7 we found that this means:  \n",
    "\n",
    "*What is the relationship between an address and an SSL (property record)?*\n",
    "\n",
    "*It is a many to many relationship. One SSL (Square, Suffix, Lot) can have multiple addresses located on it. This often includes garden style apartment complexes as well as corner addresses with separate addresses facing each adjacent street. One address can also sit upon multiple properties. One single family residence can sit upon multiple lots. The address records only contain one 'base' SSL (usually comes from OwnerPly). The cross reference table contains the many to many relationship between an address and an SSL. Some addresses do not have an associated SSL (such as some metro entrances or many addresses on Federal property). In, the address table, each address record will only be associated with one SSL. That SSL will be the base SSL, which is the SSL under which the address point is located.*\n",
    "\n",
    "We had to find a way to join the data. After analyzing uniquenes and identifiers we concluded that the  \"SSL\" attribute is our only option.\n",
    "\n",
    "We are unable to match 670 of our 107087 raw_residential_data records to the raw_address_points data. We will exclude these in from our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OBJECTID               int64\n",
       "SSL                   object\n",
       "BATHRM               float64\n",
       "HF_BATHRM            float64\n",
       "HEAT                 float64\n",
       "HEAT_D                object\n",
       "AC                    object\n",
       "NUM_UNITS            float64\n",
       "ROOMS                float64\n",
       "BEDRM                float64\n",
       "AYB                  float64\n",
       "YR_RMDL              float64\n",
       "EYB                    int64\n",
       "STORIES              float64\n",
       "SALEDATE              object\n",
       "PRICE                float64\n",
       "QUALIFIED             object\n",
       "SALE_NUM               int64\n",
       "GBA                    int64\n",
       "BLDG_NUM               int64\n",
       "STYLE                float64\n",
       "STYLE_D               object\n",
       "STRUCT               float64\n",
       "STRUCT_D              object\n",
       "GRADE                float64\n",
       "GRADE_D               object\n",
       "CNDTN                float64\n",
       "CNDTN_D               object\n",
       "EXTWALL              float64\n",
       "EXTWALL_D             object\n",
       "ROOF                 float64\n",
       "ROOF_D                object\n",
       "INTWALL              float64\n",
       "INTWALL_D             object\n",
       "KITCHENS             float64\n",
       "FIREPLACES           float64\n",
       "USECODE                int64\n",
       "LANDAREA               int64\n",
       "GIS_LAST_MOD_DTTM     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Residential Data\n",
    "\n",
    "import os\n",
    "path=\"C:/Users/danie/Documents/GitHub/DataMiningGroupProjects/Lab 3\"\n",
    "os.chdir(path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv('Data/raw_residential_data.csv')\n",
    "# df2 = pd.read_csv('Data/raw_address_points.csv')\n",
    "\n",
    "# dfj = df1.set_index('SSL').join(df2.set_index('SSL'))\n",
    "# dfj.dtypes\n",
    "# df1.info()\n",
    "# df1.describe\n",
    "df1.dtypes\n",
    "# continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3']\n",
    "# ordinal_features = ['PAY_5', 'PAY_6','default']\n",
    "# #Convert datatypes\n",
    "# df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "# df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 107,154 different residential observations with 39 different attributes. \n",
    "\n",
    "Several of the features are dummy variables of the other and are labeled with \"D\" we will use the D variables to make one hot endoding simpler to read. A dictionary of those dummy variables can be found in the appendix along with a description of each of the variables.\n",
    "\n",
    "Below we subset our variables of interest and assess the amount of NULL values and potential outliers that need to get addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b74bf2040714>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcat_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcat_drop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dfj' is not defined"
     ]
    }
   ],
   "source": [
    "#Identify all categorical variables\n",
    "categories = [['CNDTN_D','CNDTN'],['HEAT_D','HEAT'],['STYLE_D','STYLE'],['STRUCT_D','STRUCT'],['GRADE_D','GRADE'],['ROOF_D','ROOF'],['EXTWALL_D','EXTWALL'],['INTWALL_D','INTWALL']]\n",
    "cat_drop = []\n",
    "for c in categories:\n",
    "    round(dfj[c[1]])\n",
    "    cat_drop.append(c[0])\n",
    "    \n",
    "# eliminate redundant dummy variables\n",
    "dfj.drop(cat_drop, inplace=True, axis=1)\n",
    "dfj.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTID                 0\n",
      "SSL                      0\n",
      "BATHRM                  27\n",
      "HF_BATHRM               28\n",
      "HEAT                    27\n",
      "HEAT_D                  27\n",
      "AC                      27\n",
      "NUM_UNITS               27\n",
      "ROOMS                   44\n",
      "BEDRM                   31\n",
      "AYB                     13\n",
      "YR_RMDL              57708\n",
      "EYB                      0\n",
      "STORIES                 74\n",
      "SALEDATE                 0\n",
      "PRICE                19288\n",
      "QUALIFIED                0\n",
      "SALE_NUM                 0\n",
      "GBA                      0\n",
      "BLDG_NUM                 0\n",
      "STYLE                   27\n",
      "STYLE_D                 27\n",
      "STRUCT                  27\n",
      "STRUCT_D                27\n",
      "GRADE                   27\n",
      "GRADE_D                 27\n",
      "CNDTN                   27\n",
      "CNDTN_D                 27\n",
      "EXTWALL                 27\n",
      "EXTWALL_D               27\n",
      "ROOF                    27\n",
      "ROOF_D                  27\n",
      "INTWALL                 27\n",
      "INTWALL_D               27\n",
      "KITCHENS                28\n",
      "FIREPLACES              28\n",
      "USECODE                  0\n",
      "LANDAREA                 0\n",
      "GIS_LAST_MOD_DTTM        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is still 107,154 observations but now contains only 31 attributes. However, we do see that there are several null values for various features. For each of the categorical or numeric features with 100 or fewer obs we will simply impute the missing values using the most common class or median value. However, the year remodeled, & price stand out and will probably need to be treated differently.\n",
    "\n",
    "For the year remodeled we will assume that variable is missing when no remodels have been done to the home. Thus, converting the year to bins and treating the 57k with no remodel year as there own class. Clusters with a high proportion of these class homes may provides insights into homes ideal for contractors.\n",
    "\n",
    "Considering the price is the price of the last sale. Considering the volatility of the housing market over time and inflation that we believe that feature could be misleading, so we will opt to leave it out of our analysis.\n",
    "\n",
    "In addition to the price category we will also drop several other codes that we have deemed not useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1960, 1970, 1980, 1990, 2000, 2010, 2020]\n",
    "labels = ['50+','50','40','30','20','10','0']\n",
    "df1['YR_RMDL_ClASS'] = pd.cut(df1['YR_RMDL'], bins=bins, labels=labels)\n",
    "df1['YR_RMDL_ClASS'] = df1['YR_RMDL_ClASS'].replace(np.nan, 'NONE', regex=True)\n",
    "\n",
    "# eliminate unnecessary variables\n",
    "df1.drop(['PRICE','QUALIFIED','BLDG_NUM','GRADE','CNDTN','EYB','USECODE','GIS_LAST_MOD_DTTM','YR_RMDL','SALEDATE'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "df = DataFrameImputer().fit_transform(df1)\n",
    "\n",
    "int_col = ['BATHRM','HF_BATHRM','HEAT','NUM_UNITS','ROOMS','BEDRM','AYB','STORIES','STYLE','STRUCT',\n",
    "           'EXTWALL','ROOF','INTWALL','KITCHENS','FIREPLACES','LANDAREA']\n",
    "\n",
    "for i in int_col:\n",
    "    df[i] = df[i].astype('int64')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a cleaned up set of 22 different features for over 107k homes with all the missing values imputed. Before we begin analysis and clustering of our data we will explore these variables a little further to understand any transformations that may be required or any outliers that need to be addressed.\n",
    "\n",
    "Additional Outlier Vars that we determined need to be addressed:\n",
    "\n",
    "*Stories > 200*\n",
    "\n",
    "These are likely reporting errors as the there is a limit to the height of building in DC that would prevent any building being over 200 stories. This causes us to question any of the validity of the data in these observations, thus we will remove them from our data.\n",
    "\n",
    "*AYB < 1500*\n",
    "\n",
    "While it may be possible from some homes to be originally built prior to 1900, these earlier homes represent a very small proportion of our data. The majority of them have years reported as 50 or below. In order to help secure overall data quality we will remove those observations from our data. \n",
    "\n",
    "*Style > 25*\n",
    "\n",
    "Style Dummy variables greater than 25 represent two 'Styles'. Outbuildings & Vacants, neither of which we want to consider in our analysis.\n",
    "\n",
    "*Kitchens > 20*\n",
    "\n",
    "As you can see below there is only a single home with > 20 kitchens and considering the other variables this appears to be a \n",
    "reprorting error and will be removed from analyis.\n",
    "\n",
    "*Rooms > 100 or BedRooms > Rooms*\n",
    "\n",
    "Based on the square footage for the one home with > 100 rooms its seems highly unlikely that that value is accurate. Additionally, any scenario where bedrooms is greater than rooms is impossible so we will not like to use these in our analysis as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Understanding 2\n",
    "<font color=blue>Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.</font>\n",
    "\n",
    "Let explore our data a little further to better understand the features that may be of importance for our analysis. First we will try to understand some of the different Wards in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots before outlier Removals\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plotVar = ['STORIES','AYB','STYLE','KITCHENS']\n",
    "\n",
    "sns.pairplot(df[plotVar], size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal false records\n",
    "df = df[df.STORIES < 200]\n",
    "df = df[df.AYB > 1500]\n",
    "df = df[df.STYLE < 25]\n",
    "df = df[df.KITCHENS < 20]\n",
    "df = df[df.ROOMS < 100]\n",
    "df = df[df.ROOMS >= df.BEDRM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plots after\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plotVar = ['STORIES','AYB','STYLE','KITCHENS']\n",
    "\n",
    "sns.pairplot(df[plotVar], size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help the client understand the how the home types for the various neighborhoods or wards relate we have to join with data collected through a different source to get distinct location information related to these addresses. However, not all address information is available for all the homes in our set so for the purposes of this we will only consider homes which we have neighborhood information available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# subset the variables of interest that we care about\n",
    "df2 = df2.drop_duplicates(['SSL'], keep='last').set_index(\"SSL\")[[\"LATITUDE\",\"LONGITUDE\",\"ASSESSMENT_NBHD\",\"WARD\"]]\n",
    "# inner join the two datasets \n",
    "df = pd.merge(df,df2,how=\"inner\",on=\"SSL\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(df.LATITUDE, df.LONGITUDE, cmap=plt.cm.rainbow, s=2, linewidths=0)\n",
    "plt.xlabel('Latitude'), plt.ylabel('Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Modeling and Evaluation  \n",
    "\n",
    "<font color=DarkRed>Different tasks will require different evaluation methods. Be as thorough as possible when analyzing\n",
    "the data you have chosen and use visualizations of the results to explain the performance and\n",
    "expected outcomes whenever possible. Guide the reader through your analysis with plenty of\n",
    "discussion of the results.</font>\n",
    "    \n",
    "#### Option A: Cluster Analysis  \n",
    "<font color=DarkRed>  \n",
    "__Train__: Perform cluster analysis using several clustering methods (adjust parameters).  \n",
    "__Eval__: Use internal and/or external validation measures to describe and compare the clusterings and the clustersâ€” how did you determine a suitable number of clusters for each method?  \n",
    "__Visualize__: Use tables/visualization to discuss the found results. Explain each visualization in detail.  \n",
    "__Summarize__: Describe your results. What findings are the most interesting and why?</font>  \n",
    "\n",
    "#### Option B: Association Rule Mining  \n",
    "<font color=DarkRed>  \n",
    "__Train__: Create frequent itemsets and association rules (adjust parameters).  \n",
    "__Eval__: Use several measure for evaluating how interesting different rules are.  \n",
    "__Visualize__: Use tables/visualization to discuss the found results.  \n",
    "__Summarize__: Describe your results. What findings are the most compelling and why? \n",
    "</font>  \n",
    "\n",
    "#### Option C: Collaborative Filtering  \n",
    "<font color=DarkRed>  \n",
    "__Train__: Create user-item matrices or item-item matrices using collaborative filtering (adjust parameters).  \n",
    "__Eval__: Determine performance of the recommendations using different performance measures (explain the ramifications of each measure).  \n",
    "__Visualize__: Use tables/visualization to discuss the found results. Explain each visualization in detail.  \n",
    "__Summarize__: Describe your results. What findings are the most compelling and why? \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Modeling and Evaluation 1\n",
    "<font color=blue>Train and adjust parameters</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Modeling and Evaluation 2\n",
    "<font color=blue>Evaluate and Compare</font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Modeling and Evaluation 3\n",
    "<font color=blue>Visualize Results</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Modeling and Evaluation 4  \n",
    "<font color=blue>Summarize the Ramifications</font>  \n",
    "Blah blah.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Deployment\n",
    "<font color=blue>Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling? How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?</font>  \n",
    "Blah derper blah blah blah.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exceptional Work  \n",
    "<font color=blue>You have free reign to provide additional analyses or combine analyses.</font>  \n",
    "\n",
    "Blah blah. Blah blah blah BLAH!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['WARD']).OBJECTID.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['ASSESSMENT_NBHD']).OBJECTID.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
