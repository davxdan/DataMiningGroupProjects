{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Project Work Week Assignment\n",
    "### Authors: Dan Davieau, Paul Panek, Olga Tanyuk, Nathan Wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Prep\n",
    "\n",
    "For the purpose of this assignment we view the data prep as conditional to the specific classification task. However, there are several minor data cleaning tasks that are applicable to either task and are performed in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 38 columns):\n",
      "LIMIT_BAL        30000 non-null float64\n",
      "SEX              30000 non-null int64\n",
      "EDUCATION        30000 non-null int64\n",
      "MARRIAGE         30000 non-null int64\n",
      "AGE              30000 non-null int64\n",
      "PAY_0            30000 non-null int64\n",
      "PAY_2            30000 non-null int64\n",
      "PAY_3            30000 non-null int64\n",
      "PAY_4            30000 non-null int64\n",
      "PAY_5            30000 non-null int64\n",
      "PAY_6            30000 non-null int64\n",
      "BILL_AMT1        30000 non-null float64\n",
      "BILL_AMT2        30000 non-null float64\n",
      "BILL_AMT3        30000 non-null float64\n",
      "BILL_AMT4        30000 non-null float64\n",
      "BILL_AMT5        30000 non-null float64\n",
      "BILL_AMT6        30000 non-null float64\n",
      "PAY_AMT1         30000 non-null float64\n",
      "PAY_AMT2         30000 non-null float64\n",
      "PAY_AMT3         30000 non-null float64\n",
      "PAY_AMT4         30000 non-null float64\n",
      "PAY_AMT5         30000 non-null float64\n",
      "PAY_AMT6         30000 non-null float64\n",
      "default          30000 non-null int64\n",
      "log_LIMIT_BAL    30000 non-null float64\n",
      "log_PAY_AMT1     30000 non-null float64\n",
      "log_PAY_AMT2     30000 non-null float64\n",
      "log_PAY_AMT3     30000 non-null float64\n",
      "log_PAY_AMT4     30000 non-null float64\n",
      "log_PAY_AMT5     30000 non-null float64\n",
      "log_PAY_AMT6     30000 non-null float64\n",
      "log_BILL_AMT1    30000 non-null float64\n",
      "log_BILL_AMT2    30000 non-null float64\n",
      "log_BILL_AMT3    30000 non-null float64\n",
      "log_BILL_AMT4    30000 non-null float64\n",
      "log_BILL_AMT5    30000 non-null float64\n",
      "log_BILL_AMT6    30000 non-null float64\n",
      "COLLEGE          30000 non-null int32\n",
      "dtypes: float64(26), int32(1), int64(11)\n",
      "memory usage: 8.8 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('DefaultCreditcardClients.csv')\n",
    "df.rename(columns={'default payment next month':'default'}, inplace=True)\n",
    "\n",
    "#set index to the \"ID\" value and remove the ID column\n",
    "df.index = df.ID\n",
    "del df['ID']\n",
    "\n",
    "#Create Lists for Analysis\n",
    "continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3',\n",
    "                       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "                       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
    "                       'PAY_AMT6']\n",
    "ordinal_features = ['EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0','PAY_2', 'PAY_3',\n",
    "                    'PAY_4', 'PAY_5', 'PAY_6','default']\n",
    "\n",
    "#Convert datatypes\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n",
    "#convert any non-identified education categories to 'OTHER'\n",
    "df['EDUCATION'] = df['EDUCATION'].replace(to_replace=(0,5,6),value=4)\n",
    "\n",
    "#convert any non-identified marriage categories to 'OTHER'\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace(to_replace=(0),value=3)\n",
    "\n",
    "#Log transform continuous variables; as they each have a mostly \n",
    "##exponential distribution\n",
    "df[\"log_LIMIT_BAL\"]=np.log(df.LIMIT_BAL)\n",
    "df[\"log_PAY_AMT1\"]=np.log(df.PAY_AMT1+1)\n",
    "df[\"log_PAY_AMT2\"]=np.log(df.PAY_AMT2+1)\n",
    "df[\"log_PAY_AMT3\"]=np.log(df.PAY_AMT3+1)\n",
    "df[\"log_PAY_AMT4\"]=np.log(df.PAY_AMT4+1)\n",
    "df[\"log_PAY_AMT5\"]=np.log(df.PAY_AMT5+1)\n",
    "df[\"log_PAY_AMT6\"]=np.log(df.PAY_AMT6+1)\n",
    "df[\"log_BILL_AMT1\"]=np.log(df.BILL_AMT1.abs()+1)\n",
    "df[\"log_BILL_AMT2\"]=np.log(df.BILL_AMT2.abs()+1)\n",
    "df[\"log_BILL_AMT3\"]=np.log(df.BILL_AMT3.abs()+1)\n",
    "df[\"log_BILL_AMT4\"]=np.log(df.BILL_AMT4.abs()+1)\n",
    "df[\"log_BILL_AMT5\"]=np.log(df.BILL_AMT5.abs()+1)\n",
    "df[\"log_BILL_AMT6\"]=np.log(df.BILL_AMT6.abs()+1)\n",
    "\n",
    "df['COLLEGE'] = np.where(df['EDUCATION']==2, 1, 0)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Customer \"Default\"\n",
    "The first portion of the report we will explore various models to try and classify customer's likelihood of default.\n",
    "\n",
    "### Modeling & Evaluation Considerations\n",
    "\n",
    "Part 1)\n",
    "For this particular classification model we will assessing our models based on their F1 scores.\n",
    "\n",
    "A client may use this model to determine which customer-facing strategies to deploy for a given customer. If a customer is expected to default, there should be a more intesive collection strategy, and management of credit lines should be more conservative. While such strategies are designed to limit losses from defaults, they may also lead to sub-optimal customer experiences for those who do not default. Customers who are treated as likely to default are expected to attrit if they do not default.\n",
    "\n",
    "Precision is therefore important. Low precision would lead to higher customer attrition. Recall is also important. Low Recall represents missed opportunity to apply loss-mitigation strategies to customer who need them, and will result in high default costs.\n",
    "\n",
    "Since balancing these two factors is important, we will choose F1 as the evaluation metric for this suite of models.\n",
    "\n",
    "Part 2)\n",
    "In order to effectively evaluate our model performance we will splitting our data into training & test sets using an initial 80/20 split. Additionally, due to the class imbalance between default customers and non-default customers we will be oversampling default customers in our training data. If left unbalanced our model will bias towards assuming most customers will not default. This is a costly bias as described above. In order to overcome that limitation we will provide the classifiers more examples of default by sampling from replacement from our positive cases. This will likely lower our overall accuracy but will improve our "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation - Default Classification Specific\n",
    "Below we perform several steps that explain some of the data preparation tasks that are specific to the goal of classifying customers default on credit card payments.\n",
    "\n",
    "First we remove the variables that we identified as not important from Lab1 & Mini-Lab1. We also one hot encode our categorical features for use in our models. Below you will find the fields that are considered in our model.\n",
    "\n",
    "Field Definitions:\n",
    "- The items in the final data set are shown below.\n",
    "- Monetary amounts are in New Taiwanese dollars\n",
    "- SEX\n",
    "    * 1 = Male\n",
    "    * 2 = Female\n",
    "- AGE = Credit Card holder age in years\n",
    "- default: Whether or not the customer defaulted (Target for predicting default)\n",
    "- Payment history (2005)\n",
    "- PAY_0 = September\n",
    "- PAY_2 = August\n",
    "- PAY_3 = July\n",
    "- PAY_4 = June\n",
    "- PAY_5 = May\n",
    "- PAY_6 = April\n",
    "-1 = payment received on time\n",
    "    - 1 = payment received one month late\n",
    "    - 2 = payment received two months late\n",
    "    - 9 = payment received nine months late or more\n",
    "- log_LIMIT_BAL: Natural log of the Credit Limit (max amount of credit allowed)\n",
    "- log_PAY_AMT: Natural log of the amount paid by month + $1\n",
    "- log_PAY_AMT1 = September\n",
    "- log_PAY_AMT2 = August\n",
    "- log_PAY_AMT3 = July\n",
    "- log_PAY_AMT4 = June\n",
    "- log_PAY_AMT5 = May\n",
    "- log_PAY_AMT6 = April\n",
    "- EDUCATION (One-Hot Encoded when Predicting Default. Used in the form below when used as target.)\n",
    "- EDUCATION_1 = Graduate School\n",
    "- EDUCATION_2 = University\n",
    "- EDUCATION_3 = High School\n",
    "- EDUCATION_4 = Other\n",
    "- EDUCATION (This variable appears only as the target only when predicting Education. Not one-hot encoded.)\n",
    "- 1 = Graduate School\n",
    "- 2 = University\n",
    "- 3 = High School\n",
    "- 4 = Other\n",
    "- MARRIAGE (One-Hot Encoded)\n",
    "- MARRIAGE_1 = Married\n",
    "- MARRIAGE_2 = Single\n",
    "- MARRIAGE_3 = Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 23 columns):\n",
      "SEX              30000 non-null int64\n",
      "AGE              30000 non-null int64\n",
      "default          30000 non-null int64\n",
      "PAY_0            30000 non-null int64\n",
      "PAY_2            30000 non-null int64\n",
      "PAY_3            30000 non-null int64\n",
      "PAY_4            30000 non-null int64\n",
      "PAY_5            30000 non-null int64\n",
      "PAY_6            30000 non-null int64\n",
      "log_LIMIT_BAL    30000 non-null float64\n",
      "log_PAY_AMT1     30000 non-null float64\n",
      "log_PAY_AMT2     30000 non-null float64\n",
      "log_PAY_AMT3     30000 non-null float64\n",
      "log_PAY_AMT4     30000 non-null float64\n",
      "log_PAY_AMT5     30000 non-null float64\n",
      "log_PAY_AMT6     30000 non-null float64\n",
      "EDUCATION_1      30000 non-null uint8\n",
      "EDUCATION_2      30000 non-null uint8\n",
      "EDUCATION_3      30000 non-null uint8\n",
      "EDUCATION_4      30000 non-null uint8\n",
      "MARRIAGE_1       30000 non-null uint8\n",
      "MARRIAGE_2       30000 non-null uint8\n",
      "MARRIAGE_3       30000 non-null uint8\n",
      "dtypes: float64(7), int64(9), uint8(7)\n",
      "memory usage: 4.1 MB\n"
     ]
    }
   ],
   "source": [
    "#Create a separate dataset with only useful variables as identified in Lab1 and Mini-lab1.\n",
    "default = df[['SEX','EDUCATION','MARRIAGE','AGE', 'default'\n",
    "            ,'PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5','PAY_6', \"log_LIMIT_BAL\"\n",
    "            ,\"log_PAY_AMT1\",\"log_PAY_AMT2\",\"log_PAY_AMT3\",\"log_PAY_AMT4\",\"log_PAY_AMT5\"\n",
    "            ,\"log_PAY_AMT6\"]]\n",
    "\n",
    "# One-hot encoding of \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_1 = pd.get_dummies(default.EDUCATION,prefix='EDUCATION')\n",
    "tmp_df_2 = pd.get_dummies(default.MARRIAGE,prefix='MARRIAGE')\n",
    "default = pd.concat((default,tmp_df_1,tmp_df_2),axis=1)\n",
    "#Drop variables for which we used one-hot encoding\n",
    "del default['EDUCATION']\n",
    "del default['MARRIAGE']\n",
    "default.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is cleaned and the variables are prepared, we begin the process of building our training and test datasets for use in building our models. Below is we split our data into test and traing sets and fit our scaler using the training data before we oversample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training data  (24054, 23)\n",
      "Dimensions of test are  (5946, 23)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24054 entries, 2 to 30000\n",
      "Data columns (total 23 columns):\n",
      "SEX              24054 non-null int64\n",
      "AGE              24054 non-null int64\n",
      "default          24054 non-null int64\n",
      "PAY_0            24054 non-null int64\n",
      "PAY_2            24054 non-null int64\n",
      "PAY_3            24054 non-null int64\n",
      "PAY_4            24054 non-null int64\n",
      "PAY_5            24054 non-null int64\n",
      "PAY_6            24054 non-null int64\n",
      "log_LIMIT_BAL    24054 non-null float64\n",
      "log_PAY_AMT1     24054 non-null float64\n",
      "log_PAY_AMT2     24054 non-null float64\n",
      "log_PAY_AMT3     24054 non-null float64\n",
      "log_PAY_AMT4     24054 non-null float64\n",
      "log_PAY_AMT5     24054 non-null float64\n",
      "log_PAY_AMT6     24054 non-null float64\n",
      "EDUCATION_1      24054 non-null uint8\n",
      "EDUCATION_2      24054 non-null uint8\n",
      "EDUCATION_3      24054 non-null uint8\n",
      "EDUCATION_4      24054 non-null uint8\n",
      "MARRIAGE_1       24054 non-null uint8\n",
      "MARRIAGE_2       24054 non-null uint8\n",
      "MARRIAGE_3       24054 non-null uint8\n",
      "dtypes: float64(7), int64(9), uint8(7)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "split = np.random.rand(len(default)) < 0.8\n",
    "\n",
    "default_train = default[split]\n",
    "default_test = default[~split]\n",
    "\n",
    "# fit training for scaling after upsampling\n",
    "X_train = default_train.drop(columns=['default']).values     \n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "print(\"Dimensions of training data \" , default_train.shape)\n",
    "print(\"Dimensions of test are \" , default_test.shape)\n",
    "\n",
    "default_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data split and have performed the neccesary operations on the training data we can then begin building our oversampled training data then apply our scaler to the training and test data sets to begin exploring differnt estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of final training features are  (37485, 22)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_count = default.default.value_counts()\n",
    "# Class count\n",
    "default_class_0, default_class_1 = default_train.default.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "default_class_0 = default_train[default_train['default'] == 0]\n",
    "default_class_1 = default_train[default_train['default'] == 1]\n",
    "\n",
    "default_class_1_over = default_class_1.sample(frac=target_count[0]/target_count[1], replace=True)\n",
    "default_OverSampled = pd.concat([default_class_0, default_class_1_over], axis=0)\n",
    "\n",
    "#Isolate the \"default\" variable into y and keep everythign else in X to use for predictions:\n",
    "if 'default' in default_OverSampled:\n",
    "    y_train = default_OverSampled['default'].values\n",
    "    del default_OverSampled['default'] \n",
    "    X_train = default_OverSampled.values\n",
    "    \n",
    "if 'default' in default_test:\n",
    "    y_test = default_test['default'].values\n",
    "    del default_test['default'] \n",
    "    X_test = default_test.values\n",
    "\n",
    "# use the previously fit scalines to transform the data after the over sampling\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "\n",
    "print(\"Dimensions of final training features are \" , X_train_scaled.shape)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.hist(y_train )\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Observations')\n",
    "plt.title('Class Frequency after OverSampling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can see that we now have 37k observations in our training set and the labels are balanced. We can now proceed with our reviewing various estimators and how they perform when classifying customer defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Classifier Models (Part 3)\n",
    "\n",
    "For this particular problem we will compare three different models:\n",
    "\n",
    "1. Support Vector Machine\n",
    "2. K Nearest Neighbor \n",
    "3. Random Forest\n",
    "\n",
    "To ensure we are chosing the right parameters of each model we will be using sklearns GridSearchCV with a 3 fold cross validation to identify the best estimators based on the F1 scores. This provides us a dynamic means of testing out many different model parameters.\n",
    "\n",
    "We first train each model and look at some metrics for preliminary evaluation, then evaluate each model more thoroughly once all the models have been developed.\n",
    "\n",
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:  9.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator based on F1 is  SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svcEstimator = SVC()\n",
    "cv = 3\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = { 'C': [0.01, 0.1, 1]\n",
    "              ,'kernel': ['rbf','linear','poly']\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 2\n",
    "                    , scoring='f1')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n",
    "#Display the best estimator parameters\n",
    "print(\"The best estimator based on F1 is \", svcGridSearch.best_estimator_)\n",
    "svm_clf = svcGridSearch.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy 0.7803565422132526\n",
      "Confusion matrix:\n",
      "[[3818  820]\n",
      " [ 486  822]]\n",
      "f1_score: 0.5572881355932203\n",
      "Precision: 0.5006090133982948\n",
      "Recall: 0.6284403669724771\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "print(svm_clf)\n",
    "\n",
    "clf = svm_clf\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "yhat = clf.predict(X_test_scaled)\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "conf = mt.confusion_matrix(y_test,yhat)\n",
    "print('Confusion matrix:')\n",
    "print(conf)\n",
    "print('f1_score:', f1_score(y_test, yhat))\n",
    "print('Precision:', precision_score(y_test, yhat))\n",
    "print('Recall:', recall_score(y_test, yhat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  10 out of  12 | elapsed:  2.7min remaining:   32.3s\n",
      "[Parallel(n_jobs=8)]: Done  12 out of  12 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator based on F1 is  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "svcEstimator = KNeighborsClassifier()\n",
    "cv = 3\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_neighbors': [3,5,7,11]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 3\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n",
    "print(\"The best estimator based on F1 is \", svcGridSearch.best_estimator_)\n",
    "knn_clf = svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy 0.7028254288597376\n",
      "Confusion matrix:\n",
      "[[3442 1196]\n",
      " [ 571  737]]\n",
      "f1_score: 0.45479790188213515\n",
      "Precision: 0.3812726332126229\n",
      "Recall: 0.5634556574923547\n"
     ]
    }
   ],
   "source": [
    "print(knn_clf)\n",
    "\n",
    "clf = knn_clf\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "yhat = clf.predict(X_test_scaled)\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "conf = mt.confusion_matrix(y_test,yhat)\n",
    "print('Confusion matrix:')\n",
    "print(conf)\n",
    "print('f1_score:', f1_score(y_test, yhat))\n",
    "print('Precision:', precision_score(y_test, yhat))\n",
    "print('Recall:', recall_score(y_test, yhat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  36 out of  36 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator based on F1 is  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "svcEstimator = RandomForestClassifier()\n",
    "cv = 3\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_estimators': [100,250,500]\n",
    "             , 'max_depth': [5,25,50,100]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 3\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n",
    "print(\"The best estimator based on F1 is \", svcGridSearch.best_estimator_)\n",
    "rf_clf = svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8227379751093172\n",
      "Confusion matrix:\n",
      "[[4291  347]\n",
      " [ 707  601]]\n",
      "f1_score: 0.5328014184397164\n",
      "Precision: 0.6339662447257384\n",
      "Recall: 0.459480122324159\n"
     ]
    }
   ],
   "source": [
    "clf = rf_clf\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "yhat = clf.predict(X_test_scaled)\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "conf = mt.confusion_matrix(y_test,yhat)\n",
    "print('Confusion matrix:')\n",
    "print(conf)\n",
    "print('f1_score:', f1_score(y_test, yhat))\n",
    "print('Precision:', precision_score(y_test, yhat))\n",
    "print('Recall:', recall_score(y_test, yhat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Evaluation\n",
    "In the following section we evaluate the different classifiers and determine the model that we would like to use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM F1 Score  0.5572881355932203\n",
      "KNN F1 Score  0.45479790188213515\n",
      "KNN F1 Score  0.5261744966442953\n"
     ]
    }
   ],
   "source": [
    "#get the predictions from the three models for evaluation\n",
    "yhat_svm = svm_clf.predict(X_test_scaled)\n",
    "yhat_knn = knn_clf.predict(X_test_scaled)\n",
    "yhat_rf = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM F1 Score \", f1_score(y_test, yhat_svm))\n",
    "print(\"KNN F1 Score \", f1_score(y_test, yhat_knn))\n",
    "print(\"KNN F1 Score \", f1_score(y_test, yhat_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue rigorous detailed evaluation of different components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Education Level from Credit History\n",
    "The goal of these models is to try and classify a customer as having post graduate education vs not having post graduate education. \n",
    "\n",
    "### Modeling & Evaluation Considerations\n",
    "\n",
    "Part 1)\n",
    "\n",
    "One use for this suite of models is to apply marketing strategies for other products to these demographic groups if the customer does not provide the information.\n",
    "\n",
    "The evaluation metric should again consider a balance between Recall and Precision. Low Recall again represents a missed opportunity to apply a more targeted strategy, and low Precision would result in the wrong strategy applied to a given prospect.\n",
    "\n",
    "Applying the wrong strategy can be expected to decrease the credibility of the business in the customer's view, and may make the customer less likely to respond to other offers.\n",
    "\n",
    "Since F1 considers both, we will choose it as the evaluation metric for this as well.\n",
    "\n",
    "Part 2)\n",
    "\n",
    "Our cross validation technique is the same as before. We split the data into training and test sets using an 80/20 split. Using the training data to train our models and evaluating on the remaining 20. We do not have the same class imbalance problem as the other classifier so we do not have to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation - Education Classification Specific\n",
    "Below we perform several steps that explain some of the data preparation tasks that are specific to the goal of classifying education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 24 columns):\n",
      "SEX              30000 non-null int64\n",
      "COLLEGE          30000 non-null int32\n",
      "MARRIAGE         30000 non-null int64\n",
      "AGE              30000 non-null int64\n",
      "default          30000 non-null int64\n",
      "PAY_0            30000 non-null int64\n",
      "PAY_2            30000 non-null int64\n",
      "PAY_3            30000 non-null int64\n",
      "PAY_4            30000 non-null int64\n",
      "PAY_5            30000 non-null int64\n",
      "PAY_6            30000 non-null int64\n",
      "log_LIMIT_BAL    30000 non-null float64\n",
      "log_PAY_AMT1     30000 non-null float64\n",
      "log_PAY_AMT2     30000 non-null float64\n",
      "log_PAY_AMT3     30000 non-null float64\n",
      "log_PAY_AMT4     30000 non-null float64\n",
      "log_PAY_AMT5     30000 non-null float64\n",
      "log_PAY_AMT6     30000 non-null float64\n",
      "log_BILL_AMT1    30000 non-null float64\n",
      "log_BILL_AMT2    30000 non-null float64\n",
      "log_BILL_AMT3    30000 non-null float64\n",
      "log_BILL_AMT4    30000 non-null float64\n",
      "log_BILL_AMT5    30000 non-null float64\n",
      "log_BILL_AMT6    30000 non-null float64\n",
      "dtypes: float64(13), int32(1), int64(10)\n",
      "memory usage: 5.6 MB\n",
      "ShuffleSplit(n_splits=5, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "#Create a separate dataset with only useful variables as identified in Lab1 and Mini-lab1.\n",
    "educ = df[['SEX','COLLEGE','MARRIAGE','AGE', 'default'\n",
    "            ,'PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5','PAY_6', \"log_LIMIT_BAL\"\n",
    "            ,\"log_PAY_AMT1\",\"log_PAY_AMT2\",\"log_PAY_AMT3\",\"log_PAY_AMT4\",\"log_PAY_AMT5\"\n",
    "            ,\"log_PAY_AMT6\", \"log_BILL_AMT1\",\"log_BILL_AMT2\",\"log_BILL_AMT3\",\n",
    "             \"log_BILL_AMT4\",\"log_BILL_AMT5\",\"log_BILL_AMT6\"]]\n",
    "educ.info()\n",
    "\n",
    "# # # perform one-hot encoding of the categorical data \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_2 = pd.get_dummies(df.MARRIAGE,prefix='MARRIAGE')\n",
    "educ = pd.concat((educ,tmp_df_2),axis=1)\n",
    "del educ['MARRIAGE']\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "y = educ['COLLEGE'].values # get the labels we want\n",
    "del educ['COLLEGE'] # get rid of the class label\n",
    "X = educ.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 5\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training features are  (24000, 25)\n",
      "Dimensions of training target are  (24000,)\n",
      "Dimensions of testing features are  (6000, 25)\n",
      "Dimensions of testing target are  (6000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train)\n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "print(\"Dimensions of training features are \" , X_train.shape)\n",
    "print(\"Dimensions of training target are \" , y_train.shape)\n",
    "print(\"Dimensions of testing features are \" , X_test.shape)\n",
    "print(\"Dimensions of testing target are \" , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Different Classifier Models\n",
    "\n",
    "For this particular problem we will compare three different models:\n",
    "\n",
    "1. Support Vector Machine\n",
    "2. K Nearest Neighbor \n",
    "3. Random Forest\n",
    "\n",
    "To ensure we are chosing the right parameters of each model we will be using sklearns GridSearchCV with a 3 fold cross validation to identify the best estimators based on the accuracy. \n",
    "\n",
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  27 out of  27 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator based on F1 is  SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svcEstimator = SVC()\n",
    "cv = 3\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = { 'C': [0.01, 0.1, 1]\n",
    "              ,'kernel': ['rbf','linear','poly']\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 2\n",
    "                    , scoring='f1')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n",
    "#Display the best estimator parameters\n",
    "print(\"The best estimator based on F1 is \", svcGridSearch.best_estimator_)\n",
    "svm_clf2 = svcGridSearch.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy 0.6055\n",
      "Confusion matrix:\n",
      "[[2146 1041]\n",
      " [1326 1487]]\n",
      "f1_score: 0.5568245646882607\n",
      "Precision: 0.5882120253164557\n",
      "Recall: 0.5286171347316033\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "print(svm_clf2)\n",
    "\n",
    "clf = svm_clf2\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "yhat = clf.predict(X_test_scaled)\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "conf = mt.confusion_matrix(y_test,yhat)\n",
    "print('Confusion matrix:')\n",
    "print(conf)\n",
    "print('f1_score:', f1_score(y_test, yhat))\n",
    "print('Precision:', precision_score(y_test, yhat))\n",
    "print('Recall:', recall_score(y_test, yhat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  10 out of  12 | elapsed:  1.4min remaining:   16.8s\n",
      "[Parallel(n_jobs=8)]: Done  12 out of  12 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator based on F1 is  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "svcEstimator = KNeighborsClassifier()\n",
    "cv = 3\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_neighbors': [3,5,7,11]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 3\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n",
    "print(\"The best estimator based on F1 is \", svcGridSearch.best_estimator_)\n",
    "knn_clf2 = svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
      "           weights='uniform')\n",
      "Accuracy 0.575\n",
      "Confusion matrix:\n",
      "[[1964 1223]\n",
      " [1327 1486]]\n",
      "f1_score: 0.5382107931908728\n",
      "Precision: 0.5485418973791066\n",
      "Recall: 0.528261642374689\n"
     ]
    }
   ],
   "source": [
    "print(knn_clf2)\n",
    "\n",
    "clf = knn_clf2\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "yhat = clf.predict(X_test_scaled)\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "conf = mt.confusion_matrix(y_test,yhat)\n",
    "print('Confusion matrix:')\n",
    "print(conf)\n",
    "print('f1_score:', f1_score(y_test, yhat))\n",
    "print('Precision:', precision_score(y_test, yhat))\n",
    "print('Recall:', recall_score(y_test, yhat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  36 out of  36 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best estimator based on F1 is  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "svcEstimator = RandomForestClassifier()\n",
    "cv = 3\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_estimators': [100,250,500]\n",
    "             , 'max_depth': [5,25,50,100]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 3\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n",
    "print(\"The best estimator based on F1 is \", svcGridSearch.best_estimator_)\n",
    "rf_clf2 = svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=25, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Accuracy 0.6091666666666666\n",
      "Confusion matrix:\n",
      "[[2145 1042]\n",
      " [1303 1510]]\n",
      "f1_score: 0.5629077353215284\n",
      "Precision: 0.591692789968652\n",
      "Recall: 0.5367934589406328\n"
     ]
    }
   ],
   "source": [
    "print(rf_clf2)\n",
    "\n",
    "clf = rf_clf2\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)\n",
    "yhat = clf.predict(X_test_scaled)\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y_test, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "conf = mt.confusion_matrix(y_test,yhat)\n",
    "print('Confusion matrix:')\n",
    "print(conf)\n",
    "print('f1_score:', f1_score(y_test, yhat))\n",
    "print('Precision:', precision_score(y_test, yhat))\n",
    "print('Recall:', recall_score(y_test, yhat)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
