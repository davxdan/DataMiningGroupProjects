{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 20 columns):\n",
      "SEX                  30000 non-null int64\n",
      "PAY_0                30000 non-null int32\n",
      "default              30000 non-null int64\n",
      "log_LIMIT_BAL        30000 non-null float64\n",
      "log_PAY_AMT1         30000 non-null float64\n",
      "EDUCATION_1          30000 non-null uint8\n",
      "EDUCATION_2          30000 non-null uint8\n",
      "EDUCATION_3          30000 non-null uint8\n",
      "EDUCATION_4          30000 non-null uint8\n",
      "MARRIAGE_1           30000 non-null uint8\n",
      "MARRIAGE_2           30000 non-null uint8\n",
      "MARRIAGE_3           30000 non-null uint8\n",
      "AGEGROUP_0           30000 non-null uint8\n",
      "AGEGROUP_1           30000 non-null uint8\n",
      "AGEGROUP_2           30000 non-null uint8\n",
      "AGEGROUP_3           30000 non-null uint8\n",
      "AGEGROUP_4           30000 non-null uint8\n",
      "AGEGROUP_5           30000 non-null uint8\n",
      "TotalLatePayments    30000 non-null int64\n",
      "PayrateGroup         30000 non-null float64\n",
      "dtypes: float64(3), int32(1), int64(3), uint8(13)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "#import all packages for this notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('DefaultCreditcardClients.csv')\n",
    "df.rename(columns={'default payment next month':'default'}, inplace=True)\n",
    "\n",
    "#set index to the \"ID\" value and remove the ID column\n",
    "df.index = df.ID\n",
    "del df['ID']\n",
    "\n",
    "#Create Lists for Analysis\n",
    "continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3',\n",
    "                       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "                       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
    "                       'PAY_AMT6']\n",
    "ordinal_features = ['EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0','PAY_2', 'PAY_3',\n",
    "                    'PAY_4', 'PAY_5', 'PAY_6','default']\n",
    "\n",
    "#Convert datatypes\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n",
    "#convert any non-identified education categories to 'OTHER'\n",
    "df['EDUCATION'] = df['EDUCATION'].replace(to_replace=(0,5,6),value=4)\n",
    "\n",
    "#convert any non-identified marriage categories to 'OTHER'\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace(to_replace=(0),value=3)\n",
    "\n",
    "#Log transform continuous variables; as they each have a mostly \n",
    "##exponential distribution\n",
    "df[\"log_LIMIT_BAL\"]=np.log(df.LIMIT_BAL)\n",
    "df[\"log_PAY_AMT1\"]=np.log(df.PAY_AMT1+1)\n",
    "\n",
    "# #bin the ages based on various age groups \n",
    "bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "labels = [0,1,2,3,4,5]\n",
    "df['AGEGROUP'] = pd.cut(df['AGE'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# One-hot encoding of \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_1 = pd.get_dummies(df.EDUCATION,prefix='EDUCATION')\n",
    "tmp_df_2 = pd.get_dummies(df.MARRIAGE,prefix='MARRIAGE')\n",
    "tmp_df_3 = pd.get_dummies(df.AGEGROUP,prefix='AGEGROUP')\n",
    "df = pd.concat((df,tmp_df_1,tmp_df_2,tmp_df_3),axis=1)\n",
    "\n",
    "\n",
    "# flag all the payment histor to late vs not late\n",
    "payments = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "bins = [-10, 2, 10]\n",
    "labels = [0,1]\n",
    "for fi,feature in enumerate(payments):\n",
    "    df[feature] = pd.cut(df[feature], bins=bins, labels=labels).astype(np.int)\n",
    "#count how many total late payments have been made\n",
    "df['TotalLatePayments'] = df[payments].sum(axis=1)\n",
    "\n",
    "# Creating an Attribute for % of billed Amounts Paid.  Cards not used have a rate of 1000\n",
    "# Charts showing relationship of this variable to Default is in the Appendix.\n",
    "df['TotalBilled'] = df.BILL_AMT1+df.BILL_AMT2+df.BILL_AMT3+df.BILL_AMT4+df.BILL_AMT5+df.BILL_AMT5\n",
    "df['TotalPaid'] = df.PAY_AMT1+df.PAY_AMT2+df.PAY_AMT3+df.PAY_AMT4+df.PAY_AMT5+df.BILL_AMT5\n",
    "\n",
    "df['PayRateCalc']  =  df['TotalPaid']/df['TotalBilled']\n",
    "df['PayRateLimit'] = 0\n",
    "df['PayRate'] = df['PayRateCalc'].where(df['PayRateCalc'] < 1.25, 1.25)\n",
    "df['PayRate'] = df['PayRate'].where(df['TotalBilled'] > 0, 1000) # Approximately isolates Cards not used.\n",
    "df['PayRate'] = df['PayRate'].where(df['PayRate'] > 0, 0)\n",
    "\n",
    "df['PayrateGroup'] = df['PayRate']*100//5*5\n",
    "\n",
    "#Create a separate dataset in case we need to come back to original\n",
    "dfsub = df.copy()\n",
    "#dfsub = pd.concat((df,tmp_df_1,tmp_df_2),axis=1)\n",
    "\n",
    "#We will not need these attributes. We are using log of them instead.\n",
    "deleteVar = ['LIMIT_BAL','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6',\n",
    "            'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',\n",
    "            'EDUCATION','MARRIAGE','AGEGROUP','AGE','TotalBilled','TotalPaid',\n",
    "            'PayRateCalc','PayRateLimit','PayRate','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "\n",
    "for fi,feature in enumerate(deleteVar):\n",
    "    del dfsub[feature]\n",
    "\n",
    "dfsub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training features are  (24000, 19)\n",
      "Dimensions of training target are  (24000,)\n",
      "Dimensions of testing features are  (6000, 19)\n",
      "Dimensions of testing target are  (6000,)\n"
     ]
    }
   ],
   "source": [
    "#Isolate the \"default\" variable into y and keep everythign else in X to use for predictions:\n",
    "if 'default' in dfsub:\n",
    "    y = dfsub['default'].values\n",
    "    del dfsub['default'] \n",
    "    X = dfsub.values\n",
    "\n",
    "#Create a reuseable cv_object:  Random State keeps the seed.\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,test_size  = 0.2, random_state=0)\n",
    "\n",
    "#train_indices\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "print(\"Dimensions of training features are \" , X_train.shape)\n",
    "print(\"Dimensions of training target are \" , y_train.shape)\n",
    "print(\"Dimensions of testing features are \" , X_test.shape)\n",
    "print(\"Dimensions of testing target are \" , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'n_neighbors': [3, 5, 7, 10]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='accuracy',\n",
       "       verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "svcEstimator = KNeighborsClassifier()\n",
    "cv = 5\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_neighbors': [3,5,7,10]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 5\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the best estimator parameters\n",
    "svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7868333333333334\n",
      "[[4565  148]\n",
      " [1131  156]]\n"
     ]
    }
   ],
   "source": [
    "clf = svcGridSearch.best_estimator_\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "\n",
    "print('accuracy:', acc )\n",
    "print(conf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
