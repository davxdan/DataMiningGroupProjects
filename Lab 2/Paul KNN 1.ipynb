{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 20 columns):\n",
      "SEX                  30000 non-null int64\n",
      "PAY_0                30000 non-null int32\n",
      "default              30000 non-null int64\n",
      "log_LIMIT_BAL        30000 non-null float64\n",
      "log_PAY_AMT1         30000 non-null float64\n",
      "EDUCATION_1          30000 non-null uint8\n",
      "EDUCATION_2          30000 non-null uint8\n",
      "EDUCATION_3          30000 non-null uint8\n",
      "EDUCATION_4          30000 non-null uint8\n",
      "MARRIAGE_1           30000 non-null uint8\n",
      "MARRIAGE_2           30000 non-null uint8\n",
      "MARRIAGE_3           30000 non-null uint8\n",
      "AGEGROUP_0           30000 non-null uint8\n",
      "AGEGROUP_1           30000 non-null uint8\n",
      "AGEGROUP_2           30000 non-null uint8\n",
      "AGEGROUP_3           30000 non-null uint8\n",
      "AGEGROUP_4           30000 non-null uint8\n",
      "AGEGROUP_5           30000 non-null uint8\n",
      "TotalLatePayments    30000 non-null int64\n",
      "PayrateGroup         30000 non-null float64\n",
      "dtypes: float64(3), int32(1), int64(3), uint8(13)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/CreditcardDefaults.csv')\n",
    "df.rename(columns={'default payment next month':'default'}, inplace=True)\n",
    "\n",
    "#set index to the \"ID\" value and remove the ID column\n",
    "df.index = df.ID\n",
    "del df['ID']\n",
    "\n",
    "#Create Lists for Analysis\n",
    "continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3',\n",
    "                       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "                       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
    "                       'PAY_AMT6']\n",
    "ordinal_features = ['EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0','PAY_2', 'PAY_3',\n",
    "                    'PAY_4', 'PAY_5', 'PAY_6','default']\n",
    "\n",
    "#Convert datatypes\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n",
    "#convert any non-identified education categories to 'OTHER'\n",
    "df['EDUCATION'] = df['EDUCATION'].replace(to_replace=(0,5,6),value=4)\n",
    "\n",
    "#convert any non-identified marriage categories to 'OTHER'\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace(to_replace=(0),value=3)\n",
    "\n",
    "#Log transform continuous variables; as they each have a mostly \n",
    "##exponential distribution\n",
    "df[\"log_LIMIT_BAL\"]=np.log(df.LIMIT_BAL)\n",
    "df[\"log_PAY_AMT1\"]=np.log(df.PAY_AMT1+1)\n",
    "\n",
    "# #bin the ages based on various age groups \n",
    "bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "labels = [0,1,2,3,4,5]\n",
    "df['AGEGROUP'] = pd.cut(df['AGE'], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "# One-hot encoding of \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_1 = pd.get_dummies(df.EDUCATION,prefix='EDUCATION')\n",
    "tmp_df_2 = pd.get_dummies(df.MARRIAGE,prefix='MARRIAGE')\n",
    "tmp_df_3 = pd.get_dummies(df.AGEGROUP,prefix='AGEGROUP')\n",
    "df = pd.concat((df,tmp_df_1,tmp_df_2,tmp_df_3),axis=1)\n",
    "\n",
    "\n",
    "# flag all the payment histor to late vs not late\n",
    "payments = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "bins = [-10, 2, 10]\n",
    "labels = [0,1]\n",
    "for fi,feature in enumerate(payments):\n",
    "    df[feature] = pd.cut(df[feature], bins=bins, labels=labels).astype(np.int)\n",
    "#count how many total late payments have been made\n",
    "df['TotalLatePayments'] = df[payments].sum(axis=1)\n",
    "\n",
    "# Creating an Attribute for % of billed Amounts Paid.  Cards not used have a rate of 1000\n",
    "# Charts showing relationship of this variable to Default is in the Appendix.\n",
    "df['TotalBilled'] = df.BILL_AMT1+df.BILL_AMT2+df.BILL_AMT3+df.BILL_AMT4+df.BILL_AMT5+df.BILL_AMT5\n",
    "df['TotalPaid'] = df.PAY_AMT1+df.PAY_AMT2+df.PAY_AMT3+df.PAY_AMT4+df.PAY_AMT5+df.BILL_AMT5\n",
    "\n",
    "df['PayRateCalc']  =  df['TotalPaid']/df['TotalBilled']\n",
    "df['PayRateLimit'] = 0\n",
    "df['PayRate'] = df['PayRateCalc'].where(df['PayRateCalc'] < 1.25, 1.25)\n",
    "df['PayRate'] = df['PayRate'].where(df['TotalBilled'] > 0, 1000) # Approximately isolates Cards not used.\n",
    "df['PayRate'] = df['PayRate'].where(df['PayRate'] > 0, 0)\n",
    "\n",
    "df['PayrateGroup'] = df['PayRate']*100//5*5\n",
    "\n",
    "#Create a separate dataset in case we need to come back to original\n",
    "dfsub = df.copy()\n",
    "#dfsub = pd.concat((df,tmp_df_1,tmp_df_2),axis=1)\n",
    "\n",
    "#We will not need these attributes. We are using log of them instead.\n",
    "deleteVar = ['LIMIT_BAL','PAY_AMT1','PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6',\n",
    "            'BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',\n",
    "            'EDUCATION','MARRIAGE','AGEGROUP','AGE','TotalBilled','TotalPaid',\n",
    "            'PayRateCalc','PayRateLimit','PayRate','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','Unnamed: 25','Unnamed: 26']\n",
    "\n",
    "for fi,feature in enumerate(deleteVar):\n",
    "    del dfsub[feature]\n",
    "\n",
    "dfsub.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN to predict Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 23364\n",
      "Class 1: 6636\n",
      "Proportion: 3.52 : 1\n"
     ]
    }
   ],
   "source": [
    "# Target variable is unbalanced\n",
    "\n",
    "target_count = dfsub.default.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "1    23364\n",
      "0    23364\n",
      "Name: default, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create new dataset by oversampling Defaults\n",
    "#--------------------------------------------\n",
    "\n",
    "# Class count\n",
    "df_class_0, df_class_1 = dfsub.default.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = dfsub[dfsub['default'] == 0]\n",
    "df_class_1 = dfsub[dfsub['default'] == 1]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(frac=target_count[0]/target_count[1], replace=True)\n",
    "df_OverSampled = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_OverSampled.default.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Oversampled Set into CV Object, Scale X Variables\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "#Isolate the \"default\" variable into y and keep everythign else in X to use for predictions:\n",
    "if 'default' in df_OverSampled:\n",
    "    y = df_OverSampled['default'].values\n",
    "    del df_OverSampled['default'] \n",
    "    X = df_OverSampled.values\n",
    "\n",
    "#Create a reuseable cv_object:  random_state keeps the seed.\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,test_size  = 0.2, random_state=0)\n",
    "\n",
    "#train_indices\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "# Just commenting out the size-check to cut down on the number of cells.    \n",
    "#print(\"Dimensions of training features are \" , X_train.shape)\n",
    "#print(\"Dimensions of training target are \" , y_train.shape)\n",
    "#print(\"Dimensions of testing features are \" , X_test.shape)\n",
    "#print(\"Dimensions of testing target are \" , y_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:  8.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "svcEstimator = KNeighborsClassifier()\n",
    "cv = 5\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_neighbors': [3,5,7,11]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 5\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the best estimator parameters\n",
    "svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.762572223411085\n",
      "[[3019 1587]\n",
      " [ 632 4108]]\n",
      "f1_score: 0.7873502635361763\n",
      "Precision: 0.7213345039508341\n",
      "Recall: 0.8666666666666667\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import metrics as mt\n",
    "clf = svcGridSearch.best_estimator_\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "print('f1_score:', mt.f1_score(y_test, y_hat))\n",
    "print('Precision:', mt.precision_score(y_test, y_hat))\n",
    "print('Recall:', mt.recall_score(y_test, y_hat))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN To Predict Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 15970\n",
      "Class 1: 14030\n",
      "Proportion: 1.14 : 1\n"
     ]
    }
   ],
   "source": [
    "# Class count\n",
    "target_count = dfsub.EDUCATION_2.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "\n",
    "#Making judgement call that 14% imbalance is OK.  Will proceed without oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Target.  We will focus on University vs all Others.\n",
    "# Since Education is one-hot encoded, deleting the other Education One-Hots, leaving target only.\n",
    "dfEduc = dfsub\n",
    "\n",
    "del dfEduc['EDUCATION_1']\n",
    "del dfEduc['EDUCATION_3']\n",
    "del dfEduc['EDUCATION_4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Isolate the \"default\" variable into y and keep everythign else in X to use for predictions:\n",
    "if 'EDUCATION_2' in dfEduc:\n",
    "    y = dfEduc['EDUCATION_2'].values\n",
    "    del dfEduc['EDUCATION_2'] \n",
    "    X = dfEduc.values\n",
    "\n",
    "#Create a reuseable cv_object:  random_state keeps the seed.\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,test_size  = 0.2, random_state=0)\n",
    "\n",
    "#train_indices\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices] \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "# Just commenting out the size-check to cut down on the number of cells.    \n",
    "#print(\"Dimensions of training features are \" , X_train.shape)\n",
    "#print(\"Dimensions of training target are \" , y_train.shape)\n",
    "#print(\"Dimensions of testing features are \" , X_test.shape)\n",
    "#print(\"Dimensions of testing target are \" , y_test.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  20 out of  20 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "svcEstimator = KNeighborsClassifier()\n",
    "cv = 5\n",
    "\n",
    "#compare various values of C, kernels (rbf vs linear vs poly),decision_function_shape (ovo vs ovr) \n",
    "parameters = {'n_neighbors': [3,5,7,11]}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svcGridSearch = GridSearchCV(estimator=svcEstimator\n",
    "                    , n_jobs=8 # jobs to run in parallel\n",
    "                    , verbose=1 # low verbosity\n",
    "                    , param_grid=parameters\n",
    "                    , cv=cv # KFolds = 5\n",
    "                    , scoring='accuracy')\n",
    "\n",
    "svcGridSearch.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=11, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the best estimator parameters\n",
    "svcGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5666666666666667\n",
      "[[1978 1208]\n",
      " [1392 1422]]\n",
      "f1_score: 0.5224099926524615\n",
      "Precision: 0.5406844106463878\n",
      "Recall: 0.5053304904051172\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn import metrics as mt\n",
    "clf = svcGridSearch.best_estimator_\n",
    "\n",
    "clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "print('f1_score:', mt.f1_score(y_test, y_hat))\n",
    "print('Precision:', mt.precision_score(y_test, y_hat))\n",
    "print('Recall:', mt.recall_score(y_test, y_hat))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Meaning Type\n",
    "\n",
    "#### Attribute Information\n",
    "The data used is \"Default of Credit Card Clients\" from UCI. It was attained by I-Cheng Yeh with Chung Hua University and Tamkang University in Taiwan. The original goal was to predict default rates.\n",
    "\n",
    "The data has a 6 month history of 30,000 Taiwanese credit account balances and transactions. Each observation contains a binary reponse variable \"default\" with values 1 indicating a default occured and 0 indicating no default occured.\n",
    "\n",
    "The following explanatory variables are included:\n",
    "\n",
    " - LIMIT_BAL = Total credit amount allowed\n",
    " \n",
    " - SEX\n",
    "     -  1 = Male\n",
    "     -  2 = Female\n",
    " \n",
    " - EDUCATION\n",
    "     - 1 = Graduate School\n",
    "     - 2 = University\n",
    "     - 3 = High School\n",
    "     - 4 = Other\n",
    "   \n",
    " - MARRIAGE\n",
    "     - 1 = Married\n",
    "     - 2 = Single\n",
    "     - 3 = Other\n",
    "       \n",
    " - AGE = Credit holder age in years\n",
    " \n",
    "Payment history (2005)\n",
    " - PAY_0 = September\n",
    " - PAY_2 = August\n",
    " - PAY_3 = July\n",
    " - PAY_4 = June\n",
    " - PAY_5 = May\n",
    " - PAY_6 = April\n",
    "      -  -1 = payment received on time\n",
    "      -   1 = payment received one month late\n",
    "      -   2 = payment received two months late\n",
    "      -   \"......\"\n",
    "      -   9 = payment received nine months late or more\n",
    "         \n",
    "Statement amount (NT dollars, 2005)\n",
    " - BILL_AMT1 = September\n",
    " - BILL_AMT2 = August\n",
    " - BILL_AMT3 = July\n",
    " - BILL_AMT4 = June\n",
    " - BILL_AMT5 = May\n",
    " - BILL_AMT6 = April\n",
    " \n",
    "Payment amount (NT dollars, 2005).\n",
    " - PAY_AMT1 = September\n",
    " - PAY_AMT2 = August\n",
    " - PAY_AMT3 = July\n",
    " - PAY_AMT4 = June\n",
    " - PAY_AMT5 = May\n",
    " - PAY_AMT6 = April\n",
    "\n",
    "Original Source Data Set Information  \n",
    "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
