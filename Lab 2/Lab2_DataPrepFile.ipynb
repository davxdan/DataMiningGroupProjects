{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Prep of Master Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 18 columns):\n",
      "SEX              30000 non-null int64\n",
      "EDUCATION        30000 non-null int64\n",
      "MARRIAGE         30000 non-null int64\n",
      "AGE              30000 non-null int64\n",
      "default          30000 non-null int64\n",
      "PAY_0            30000 non-null int64\n",
      "PAY_2            30000 non-null int64\n",
      "PAY_3            30000 non-null int64\n",
      "PAY_4            30000 non-null int64\n",
      "PAY_5            30000 non-null int64\n",
      "PAY_6            30000 non-null int64\n",
      "log_LIMIT_BAL    30000 non-null float64\n",
      "log_PAY_AMT1     30000 non-null float64\n",
      "log_PAY_AMT2     30000 non-null float64\n",
      "log_PAY_AMT3     30000 non-null float64\n",
      "log_PAY_AMT4     30000 non-null float64\n",
      "log_PAY_AMT5     30000 non-null float64\n",
      "log_PAY_AMT6     30000 non-null float64\n",
      "dtypes: float64(7), int64(11)\n",
      "memory usage: 4.3 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('DefaultCreditcardClients.csv')\n",
    "df.rename(columns={'default payment next month':'default'}, inplace=True)\n",
    "\n",
    "#set index to the \"ID\" value and remove the ID column\n",
    "df.index = df.ID\n",
    "del df['ID']\n",
    "\n",
    "#Create Lists for Analysis\n",
    "continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3',\n",
    "                       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "                       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
    "                       'PAY_AMT6']\n",
    "ordinal_features = ['EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0','PAY_2', 'PAY_3',\n",
    "                    'PAY_4', 'PAY_5', 'PAY_6','default']\n",
    "\n",
    "#Convert datatypes\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n",
    "#convert any non-identified education categories to 'OTHER'\n",
    "df['EDUCATION'] = df['EDUCATION'].replace(to_replace=(0,5,6),value=4)\n",
    "\n",
    "#convert any non-identified marriage categories to 'OTHER'\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace(to_replace=(0),value=3)\n",
    "\n",
    "#Log transform continuous variables; as they each have a mostly \n",
    "##exponential distribution\n",
    "df[\"log_LIMIT_BAL\"]=np.log(df.LIMIT_BAL)\n",
    "df[\"log_PAY_AMT1\"]=np.log(df.PAY_AMT1+1)\n",
    "df[\"log_PAY_AMT2\"]=np.log(df.PAY_AMT2+1)\n",
    "df[\"log_PAY_AMT3\"]=np.log(df.PAY_AMT3+1)\n",
    "df[\"log_PAY_AMT4\"]=np.log(df.PAY_AMT4+1)\n",
    "df[\"log_PAY_AMT5\"]=np.log(df.PAY_AMT5+1)\n",
    "df[\"log_PAY_AMT6\"]=np.log(df.PAY_AMT6+1)\n",
    "\n",
    "#Create a separate dataset with only useful variables as identified in Lab1 and Mini-lab1.\n",
    "df = df[['SEX','EDUCATION','MARRIAGE','AGE', 'default'\n",
    "            ,'PAY_0','PAY_2', 'PAY_3', 'PAY_4', 'PAY_5','PAY_6', \"log_LIMIT_BAL\"\n",
    "            ,\"log_PAY_AMT1\",\"log_PAY_AMT2\",\"log_PAY_AMT3\",\"log_PAY_AMT4\",\"log_PAY_AMT5\"\n",
    "            ,\"log_PAY_AMT6\"]]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Customer \"Default\"\n",
    "I think it is easier to do the dependent variables seperately since we oversample with the default variable at first, so we are going to be working with a different dat set for this one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Specific data prep\n",
    "##### Split data & over sample on the minority default class to help improve F1 statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training data  (24168, 23)\n",
      "Dimensions of test are  (5832, 23)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24168 entries, 1 to 30000\n",
      "Data columns (total 23 columns):\n",
      "SEX              24168 non-null int64\n",
      "AGE              24168 non-null int64\n",
      "default          24168 non-null int64\n",
      "PAY_0            24168 non-null int64\n",
      "PAY_2            24168 non-null int64\n",
      "PAY_3            24168 non-null int64\n",
      "PAY_4            24168 non-null int64\n",
      "PAY_5            24168 non-null int64\n",
      "PAY_6            24168 non-null int64\n",
      "log_LIMIT_BAL    24168 non-null float64\n",
      "log_PAY_AMT1     24168 non-null float64\n",
      "log_PAY_AMT2     24168 non-null float64\n",
      "log_PAY_AMT3     24168 non-null float64\n",
      "log_PAY_AMT4     24168 non-null float64\n",
      "log_PAY_AMT5     24168 non-null float64\n",
      "log_PAY_AMT6     24168 non-null float64\n",
      "EDUCATION_1      24168 non-null uint8\n",
      "EDUCATION_2      24168 non-null uint8\n",
      "EDUCATION_3      24168 non-null uint8\n",
      "EDUCATION_4      24168 non-null uint8\n",
      "MARRIAGE_1       24168 non-null uint8\n",
      "MARRIAGE_2       24168 non-null uint8\n",
      "MARRIAGE_3       24168 non-null uint8\n",
      "dtypes: float64(7), int64(9), uint8(7)\n",
      "memory usage: 3.3 MB\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding of \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_1 = pd.get_dummies(df.EDUCATION,prefix='EDUCATION')\n",
    "tmp_df_2 = pd.get_dummies(df.MARRIAGE,prefix='MARRIAGE')\n",
    "dfsub1 = pd.concat((df,tmp_df_1,tmp_df_2),axis=1)\n",
    "#Drop variables for which we used one-hot encoding\n",
    "del dfsub1['EDUCATION']\n",
    "del dfsub1['MARRIAGE']\n",
    "\n",
    "\n",
    "split = np.random.rand(len(dfsub1)) < 0.8\n",
    "\n",
    "df_train = dfsub1[split]\n",
    "df_test = dfsub1[~split]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# fit training for scaling after upsampling\n",
    "X_train = df_train.drop(columns=['default']).values     \n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "print(\"Dimensions of training data \" , df_train.shape)\n",
    "print(\"Dimensions of test are \" , df_test.shape)\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random over-sampling:\n",
      "0    18784\n",
      "1    18667\n",
      "Name: default, dtype: int64\n",
      "Dimensions of training features are  (37451, 22)\n",
      "Dimensions of training target are  (37451,)\n",
      "Dimensions of testing features are  (5914, 22)\n",
      "Dimensions of testing target are  (5914,)\n"
     ]
    }
   ],
   "source": [
    "target_count = dfsub.default.value_counts()\n",
    "# Class count\n",
    "df_class_0, df_class_1 = df_train.default.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = df_train[df_train['default'] == 0]\n",
    "df_class_1 = df_train[df_train['default'] == 1]\n",
    "\n",
    "df_class_1_over = df_class_1.sample(frac=target_count[0]/target_count[1], replace=True)\n",
    "df_OverSampled = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "print('Random over-sampling:')\n",
    "print(df_OverSampled.default.value_counts())\n",
    "\n",
    "\n",
    "#Isolate the \"default\" variable into y and keep everythign else in X to use for predictions:\n",
    "if 'default' in df_OverSampled:\n",
    "    y_train = df_OverSampled['default'].values\n",
    "    del df_OverSampled['default'] \n",
    "    X_train = df_OverSampled.values\n",
    "    \n",
    "if 'default' in df_test:\n",
    "    y_test = df_test['default'].values\n",
    "    del df_test['default'] \n",
    "    X_test = df_test.values\n",
    "\n",
    "print(\"Dimensions of training features are \" , X_train.shape)\n",
    "print(\"Dimensions of training target are \" , y_train.shape)\n",
    "print(\"Dimensions of testing features are \" , X_test.shape)\n",
    "print(\"Dimensions of testing target are \" , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### apply scales to final training & test set to begin exploring models for Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the previously fit scalines to transform the data after the over sampling\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert model, visualizations, & interpretation for \"Default Predictions here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Education from Credit History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=5, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# perform one-hot encoding of the categorical data \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_2 = pd.get_dummies(df.MARRIAGE,prefix='MARRIAGE')\n",
    "dfsub2 = pd.concat((df,tmp_df_2),axis=1)\n",
    "del dfsub2['MARRIAGE']\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'default' in dfsub2:\n",
    "    y = dfsub2['EDUCATION'].values # get the labels we want\n",
    "    del dfsub2['EDUCATION'] # get rid of the class label\n",
    "    X = dfsub2.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 5\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training features are  (24000, 19)\n",
      "Dimensions of training target are  (24000,)\n",
      "Dimensions of testing features are  (6000, 19)\n",
      "Dimensions of testing target are  (6000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scl_obj = StandardScaler()\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train)\n",
    "X_test_scaled = scl_obj.transform(X_test)\n",
    "\n",
    "print(\"Dimensions of training features are \" , X_train.shape)\n",
    "print(\"Dimensions of training target are \" , y_train.shape)\n",
    "print(\"Dimensions of testing features are \" , X_test.shape)\n",
    "print(\"Dimensions of testing target are \" , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert models for education here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
