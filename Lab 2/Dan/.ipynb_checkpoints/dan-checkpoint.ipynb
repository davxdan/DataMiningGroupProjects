{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over Sample and undersample? Bootstrap? repeated holdout? Stratefied k Fold Scikit learn has weights feature\n",
    "\n",
    "We are interested in precision for no default, recall for default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('DefaultCreditcardClients.csv')\n",
    "df.rename(columns={'default payment next month':'default'}, inplace=True)\n",
    "df.index = df.ID\n",
    "if 'ID' in df:\n",
    "    del df['ID']\n",
    "df[\"log_LIMIT_BAL\"]=np.log(df.LIMIT_BAL)\n",
    "df[\"log_PAY_AMT1\"]=np.log(df.PAY_AMT1+1)\n",
    "df[\"log_PAY_AMT2\"]=np.log(df.PAY_AMT2+1)\n",
    "df[\"log_PAY_AMT3\"]=np.log(df.PAY_AMT3+1)\n",
    "df[\"log_PAY_AMT4\"]=np.log(df.PAY_AMT4+1)\n",
    "df[\"log_PAY_AMT5\"]=np.log(df.PAY_AMT5+1)\n",
    "df[\"log_PAY_AMT6\"]=np.log(df.PAY_AMT6+1)\n",
    "continuous_features = ['LIMIT_BAL', 'BILL_AMT1', 'BILL_AMT2','BILL_AMT3',\n",
    "                       'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
    "                       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5',\n",
    "                       'PAY_AMT6']\n",
    "ordinal_features = ['EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0','PAY_2', 'PAY_3',\n",
    "                    'PAY_4', 'PAY_5', 'PAY_6','default']\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n",
    "#convert any non-identified education categories to 'OTHER'\n",
    "df['EDUCATION'] = df['EDUCATION'].replace(to_replace=(0,5,6),value=4)\n",
    "\n",
    "#convert any non-identified marriage categories to 'OTHER'\n",
    "df['MARRIAGE'] = df['MARRIAGE'].replace(to_replace=(0),value=3)\n",
    "\n",
    "# #bin the ages based on various age groups \n",
    "bins = [18, 25, 35, 45, 55, 65, 100]\n",
    "labels = [0,1,2,3,4,5]\n",
    "df['AGEGROUP'] = pd.cut(df['AGE'], bins=bins, labels=labels)\n",
    "\n",
    "# # One-hot encoding of \"EDUCATION\" and \"MARRIAGE\".\n",
    "tmp_df_1 = pd.get_dummies(df.EDUCATION,prefix='EDUCATION')\n",
    "tmp_df_2 = pd.get_dummies(df.MARRIAGE,prefix='MARRIAGE')\n",
    "tmp_df_3 = pd.get_dummies(df.AGEGROUP,prefix='AGEGROUP')\n",
    "df = pd.concat((df,tmp_df_1,tmp_df_2,tmp_df_3),axis=1)\n",
    "\n",
    "# flag all the payment histor to late vs not late\n",
    "payments = ['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']\n",
    "bins = [-10, 2, 10]\n",
    "labels = [0,1]\n",
    "for fi,feature in enumerate(payments):\n",
    "    df[feature] = pd.cut(df[feature], bins=bins, labels=labels).astype(np.int)\n",
    "#count how many total late payments have been made\n",
    "df['TotalLatePayments'] = df[payments].sum(axis=1)\n",
    "\n",
    "# # Creating an Attribute for % of billed Amounts Paid.  Cards not used have a rate of 1000\n",
    "# # Charts showing relationship of this variable to Default is in the Appendix.\n",
    "df['TotalBilled'] = df.BILL_AMT1+df.BILL_AMT2+df.BILL_AMT3+df.BILL_AMT4+df.BILL_AMT5+df.BILL_AMT5\n",
    "df['TotalPaid'] = df.PAY_AMT1+df.PAY_AMT2+df.PAY_AMT3+df.PAY_AMT4+df.PAY_AMT5+df.BILL_AMT5\n",
    "\n",
    "df['PayRateCalc']  =  df['TotalPaid']/df['TotalBilled']\n",
    "df['PayRateLimit'] = 0\n",
    "df['PayRate'] = df['PayRateCalc'].where(df['PayRateCalc'] < 1.25, 1.25)\n",
    "df['PayRate'] = df['PayRate'].where(df['TotalBilled'] > 0, 1000) # Approximately isolates Cards not used.\n",
    "df['PayRate'] = df['PayRate'].where(df['PayRate'] > 0, 0)\n",
    "\n",
    "df['PayrateGroup'] = df['PayRate']*100//5*5\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (Random Forests)\n",
    "- Boosting (Gradient of Error)\n",
    "- Statistical Comparison Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes: 30000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGTRJREFUeJzt3Xu0ZGV95vHvQwPiBQQEGUZIWlmtBl1qtFUUYyQqIjqiS406Kq0ydqJoNJo4GM3gqJnxEnUk8RLUFsjygpcYehTFHkRxVJRGuaqEFlF7QGgD4gVv4G/+2O+RoqlzzqZ716ku+vtZq1btemtffu+p7vOcfal3p6qQJGkIO0y7AEnSrYehIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRrMjtMuYKnttddetXz58mmXIUkz5ZxzzvlRVe292HzbXagsX76c9evXT7sMSZopSb7XZz4Pf0mSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgaz3X2jfmssP+ZTU9nuZW943FS2K0m3lHsqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBGCqSpMEYKpKkwRgqkqTBTCxUkuyf5Iwk30pyUZKXtPY9k6xLckl73qO1J8lxSTYkOT/J/UfWtarNf0mSVSPtD0hyQVvmuCSZVH8kSYub5J7K9cDLq+oPgIOAo5McCBwDnF5VK4DT22uAxwIr2mM18C7oQgg4Fngw8CDg2LkgavOsHlnusAn2R5K0iImFSlVdUVVfb9M/Bb4F3AU4AjixzXYi8MQ2fQRwUnXOAnZPsi/wGGBdVV1dVdcA64DD2nu7VdVXqqqAk0bWJUmagiU5p5JkOfCHwFeBfarqCuiCB7hzm+0uwA9GFtvY2hZq3zimXZI0JRMPlSR3AD4OvLSqfrLQrGPaagvax9WwOsn6JOs3bdq0WMmSpC000VBJshNdoHygqv6lNV/ZDl3Rnq9q7RuB/UcW3w+4fJH2/ca030xVHV9VK6tq5d577711nZIkzWuSV38FeB/wrap668hba4G5K7hWAaeMtB/ZrgI7CLi2HR47DTg0yR7tBP2hwGntvZ8mOaht68iRdUmSpmDHCa77YODZwAVJzm1tfwO8AfhIkqOA7wNPbe+dChwObACuA54LUFVXJ3kdcHab77VVdXWbfgFwAnBb4NPtIUmakomFSlX9X8af9wB45Jj5Czh6nnWtAdaMaV8P3HsrypQkDchv1EuSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgZjqEiSBmOoSJIGY6hIkgazaKgk+fsk91qKYiRJs63Pnsq3geOTfDXJnye546SLkiTNpkVDpareW1UHA0cCy4Hzk3wwySGTLk6SNFt6nVNJsgy4Z3v8CDgPeFmSD0+wNknSjNlxsRmSvBV4AnA68D+q6mvtrTcmuXiSxUmSZsuioQJcCLy6qq4b896DBq5HkjTD+hz+ugbYae5Fkt2TPBGgqq6dVGGSpNnTJ1SOHQ2PqvoxcOzkSpIkzao+oTJunj6HzSRJ25k+obI+yVuTHJDkbkneBpwz6cIkSbOnT6i8GPg1cDLwUeCXwNGTLEqSNJv6fPnx51V1TFWtrKoHVNUrq+rniy2XZE2Sq5JcONL2miT/L8m57XH4yHuvTLIhycVJHjPSflhr25DkmJH2u7Zv+V+S5OQkO9+yrkuShtZn7K+7Jzk+yWeTfG7u0WPdJwCHjWl/W1Xdrz1Obds4EHg6cK+2zDuTLGtfunwH8FjgQOAZbV6AN7Z1raC7Qu2oHjVJkiaozwn3jwLvBt4L3NB3xVV1ZpLlPWc/AvhwVf0K+G6SDdz4HZgNVXUpQPsG/xFJvgX8CfCf2zwnAq8B3tW3PknS8PqEyvVVNeQv6xclORJYD7y8qq4B7gKcNTLPxtYG8IPN2h8M3An4cVVdP2Z+SdKU9DlR/7+TvDDJvkn2nHts4fbeBRwA3A+4AnhLa8+YeWsL2sdKsjrJ+iTrN23adMsqliT11mdPZVV7/uuRtgLudks3VlVXzk0neQ/wyfZyI7D/yKz7AZe36XHtPwJ2T7Jj21sZnX/cdo8HjgdYuXLlvOEjSdo6fa7+uuuYxy0OFIAk+468fBLduGIAa4GnJ7lNkrsCK4CvAWcDK9qVXjvTncxfW1UFnAE8pS2/CjhlS2qSJA2nzyjFtwNeBvxeVa1OsgK4R1V9cpHlPgQ8AtgryUa6oV0ekeR+dHs6lwF/BlBVFyX5CPBN4Hrg6Kq6oa3nRcBpwDJgTVVd1DbxX4EPJ3k98A3gfbek45Kk4fU5/PV+um/QP7S93kh3RdiCoVJVzxjTPO8v/qr6O+DvxrSfCpw6pv1SHCVZkrYpfU7UH1BVbwJ+A1BVv2D8iXJJ0nauT6j8OsltaVdXJTkA+NVEq5IkzaQ+h7+OBT4D7J/kA8DBwHMmWZQkaTYtGipVtS7J14GD6A57vaSqfjTxyiRJM6fP1V8Pb5M/bc8HJqGqzpxcWZKkWdTn8Nfolx53obvi6hy6sbckSfqdPoe//tPo6yT7A2+aWEWSpJnV5+qvzW0E7j10IZKk2dfnnMo/cONgjTvQDQZ53iSLkiTNpj7nVNaPTF8PfKiqvjSheiRJM6zPOZUTl6IQSdLs63P46wLG36skQFXVfQavSpI0k/oc/vp0e/7n9vxM4Dq6W/hKkvQ7fULl4Ko6eOT1MUm+VFWvnVRRkqTZ1OeS4tsnedjciyQPBW4/uZIkSbOqz57KUcCaJHekO7dyLfC8iVYlSZpJfa7+Oge4b5LdgFTVtZMvS5I0ixY9/JVknyTvA06uqmuTHJjkqCWoTZI0Y/qcUzmB7h7x/7G9/jfgpZMqSJI0u/qEyl5V9RHgtwBVdT1ww0SrkiTNpD6h8vMkd+LG2wkfRHeyXpKkm+hz9dfLgLXAAUm+BOwNPGWiVUmSZtKCoZJkB7obc/0xcA+6oVkurqrfLEFtkqQZs2CoVNVvk7ylqh4CXLRENUmSZlSfcyqfTfLkJJl4NZKkmdb3nMrtgeuT/JIbRyfebaKVSZJmzrx7KknmBpHcu6p2qKqdq2q3qtrVQJEkjbPQ4a/j2vOXl6IQSdLsW+jw12+SvB/YL8lxm79ZVX8xubIkSbNooVB5PPAo4E+Ac5amHEnSLJs3VKrqR8CHk3yrqs5bwpokSTNq0UuKDRRJUl99vqciSVIvhookaTB9btL1kiS7pfO+JF9PcuhSFCdJmi199lSeV1U/AQ6lG6H4ucAbJlqVJGkm9QmVuTG/Dgfe307cLzoOWJI1Sa5KcuFI255J1iW5pD3v0dqT5LgkG5Kcn+T+I8usavNfkmTVSPsDklzQljnOsckkafr6hMo5ST5LFyqnJdmVdhfIRZwAHLZZ2zHA6VW1Aji9vQZ4LLCiPVYD74IuhIBjgQcDDwKOnQuiNs/qkeU235YkaYn1CZWj6H75P7CqrgN2ojsEtqCqOhO4erPmI4AT2/SJwBNH2k+qzlnA7kn2BR4DrKuqq6vqGmAdcFh7b7eq+kpVFXDSyLokSVPSJ1QeQndjrh8neRbwarb8dsL7VNUVAO35zq39LsAPRubb2NoWat84pn2sJKuTrE+yftOmTVtYuiRpMX1C5V3AdUnuC7wC+B7dnsGQxp0PqS1oH6uqjq+qlVW1cu+9997CEiVJi+kTKte3Q0xHAG+vqrcDu27h9q5sh65oz1e19o3A/iPz7Qdcvkj7fmPaJUlT1CdUfprklcCzgE8lWUZ3XmVLrAXmruBaBZwy0n5kuwrsIODadnjsNODQJHu0E/SHAqe1936a5KB21deRI+uSJE1Jn1B5GvAr4Kiq+iHduYs3L7ZQkg8BXwHukWRjkqPovt/y6CSXAI/mxu+7nApcCmwA3gO8EKCqrgZeB5zdHq9tbQAvAN7blvkO8OkefZEkTdCitxNuQfLWkdffp8c5lap6xjxvPXLMvAUcPc961gBrxrSvB+69WB2SpKXTZ5iWg5KcneRnSX6d5IYkW3r1lyTpVqzP4a9/BJ4BXALcFvgvwDsmWZQkaTYtevgLoKo2JFlWVTcA70/ifeslSTfTJ1SuS7IzcG6SNwFXALefbFmSpFnU5/DXs4FlwIuAn9N9b+TJkyxKkjSb+lz99b02+Qvgv0+2HEnSLJs3VJJcwMJDn9xnIhVJkmbWQnsqj1+yKiRJtwoLhcpOdKMKf2m0Mckf4ThbkqQxFjpR/7+An45p/0V7T5Kkm1goVJZX1fmbN7bhUZZPrCJJ0sxaKFR2WeC92w5diCRp9i0UKmcnef7mjW204XMmV5IkaVYtdKL+pcAnkjyTG0NkJbAz8KRJFyZJmj3zhkpVXQk8NMkh3DjE/Keq6nNLUpkkaeb0+Ub9GcAZS1CLJGnG9Rn7S5KkXgwVSdJgDBVJ0mAMFUnSYAwVSdJgDBVJ0mAMFUnSYAwVSdJgFv3yoyRpOMuP+dRUtnvZGx63JNtxT0WSNBhDRZI0GENFkjQYQ0WSNBhDRZI0GENFkjQYQ0WSNBhDRZI0GENFkjQYQ0WSNJiphEqSy5JckOTcJOtb255J1iW5pD3v0dqT5LgkG5Kcn+T+I+tZ1ea/JMmqafRFknSjae6pHFJV96uqle31McDpVbUCOL29BngssKI9VgPvgi6EgGOBBwMPAo6dCyJJ0nRsS4e/jgBObNMnAk8caT+pOmcBuyfZF3gMsK6qrq6qa4B1wGFLXbQk6UbTCpUCPpvknCSrW9s+VXUFQHu+c2u/C/CDkWU3trb52m8myeok65Os37Rp04DdkCSNmtbQ9wdX1eVJ7gysS/LtBebNmLZaoP3mjVXHA8cDrFy5cuw8kqStN5U9laq6vD1fBXyC7pzIle2wFu35qjb7RmD/kcX3Ay5foF2SNCVLHipJbp9k17lp4FDgQmAtMHcF1yrglDa9FjiyXQV2EHBtOzx2GnBokj3aCfpDW5skaUqmcfhrH+ATSea2/8Gq+kySs4GPJDkK+D7w1Db/qcDhwAbgOuC5AFV1dZLXAWe3+V5bVVcvXTckSZtb8lCpqkuB+45p/3fgkWPaCzh6nnWtAdYMXaMkactsS5cUS5JmnKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGoyhIkkajKEiSRrMzIdKksOSXJxkQ5Jjpl2PJG3PZjpUkiwD3gE8FjgQeEaSA6dblSRtv2Y6VIAHARuq6tKq+jXwYeCIKdckSdutWQ+VuwA/GHm9sbVJkqZgx2kXsJUypq1uNlOyGljdXv4sycVbuL29gB9t4bJbLG9c6i3exFT6PGX2+dZve+sveeNW9/n3+8w066GyEdh/5PV+wOWbz1RVxwPHb+3GkqyvqpVbu55ZYp+3D9tbn7e3/sLS9XnWD3+dDaxIctckOwNPB9ZOuSZJ2m7N9J5KVV2f5EXAacAyYE1VXTTlsiRpuzXToQJQVacCpy7R5rb6ENoMss/bh+2tz9tbf2GJ+pyqm53XliRpi8z6ORVJ0jbEUBljsaFfktwmycnt/a8mWb70VQ6nR39fluSbSc5PcnqSXpcWbsv6Du+T5ClJKsnMXynUp89J/rR91hcl+eBS1zi0Hv+2fy/JGUm+0f59Hz6NOoeSZE2Sq5JcOM/7SXJc+3mcn+T+gxdRVT5GHnQn/L8D3A3YGTgPOHCzeV4IvLtNPx04edp1T7i/hwC3a9MvmOX+9u1zm29X4EzgLGDltOtegs95BfANYI/2+s7TrnsJ+nw88II2fSBw2bTr3so+Pxy4P3DhPO8fDnya7jt+BwFfHboG91Rurs/QL0cAJ7bpjwGPTDLui5izYNH+VtUZVXVde3kW3feBZlnf4X1eB7wJ+OVSFjchffr8fOAdVXUNQFVdtcQ1Dq1PnwvYrU3fkTHfc5slVXUmcPUCsxwBnFSds4Ddk+w7ZA2Gys31Gfrld/NU1fXAtcCdlqS64d3SoW6OovtLZ5Yt2uckfwjsX1WfXMrCJqjP53x34O5JvpTkrCSHLVl1k9Gnz68BnpVkI91VpC9emtKmZuJDW838JcUT0Gfol17Dw8yI3n1J8ixgJfDHE61o8hbsc5IdgLcBz1mqgpZAn895R7pDYI+g2xv9YpJ7V9WPJ1zbpPTp8zOAE6rqLUkeAvxz6/NvJ1/eVEz8d5d7KjfXZ+iX382TZEe63eaFdjm3Zb2GuknyKOBVwBOq6ldLVNukLNbnXYF7A59Pchndsee1M36yvu+/61Oq6jdV9V3gYrqQmVV9+nwU8BGAqvoKsAvduGC3Vr3+v28NQ+Xm+gz9shZY1aafAnyu2lmwGbRof9uhoH+iC5RZP84Oi/S5qq6tqr2qanlVLac7j/SEqlo/nXIH0eff9b/SXZRBkr3oDodduqRVDqtPn78PPBIgyR/QhcqmJa1yaa0FjmxXgR0EXFtVVwy5AQ9/babmGfolyWuB9VW1Fngf3W7yBro9lKdPr+Kt07O/bwbuAHy0XY/w/ap6wtSK3ko9+3yr0rPPpwGHJvkmcAPw11X179Oreuv07PPLgfck+Uu6w0DPmeE/EEnyIbrDl3u180THAjsBVNW76c4bHQ5sAK4Dnjt4DTP885MkbWM8/CVJGoyhIkkajKEiSRqMoSJJGoyhIkkajKGibUaS/5Dkw0m+00bKPTXJ3ZMsn2/U1aWW5DVJ/mqgdd02yReSLBtifUshyVOTfCvJGQOu80VJBr+0VdNhqGib0Abk/ATw+ao6oKoOBP4G2Ge6lU3U84B/qaobpl3IOPOE3VHAC6vqkAE3tQb4iwHXpykyVLStOAT4TfuCFgBVdW5VfXF0prbX8sUkX2+Ph7b2fZOcmeTcJBcm+aMky5Kc0F5f0L7gNrquOya5rI31RZLbJflBkp2SPD/J2UnOS/LxJLfbvOAkn58buiXJXm1IF9p239yWPz/Jn83T52cCp7Rl0paZq/Vprf0RbTsfS/LtJB+YGxE7yQPans45SU7LmNFmW//f3X5m/5bk8QvV2LZ3Rrp7qVyw2br+G/Aw4N1t2bGfRZv3Fa0f5yV5Q2s7IMlnWr1fTHLP9jlfB1yW5EHz/Jw0S6Y9/r8PH1UF3V+qb5vnveW0+0MAtwN2adMr6L4ZDd03o1/VppfRjd/1AGDdyHp2H7PuU4BD2vTTgPe26TuNzPN64MVt+jXAX7Xpz9Pus0I3XtRlbXo18Oo2fRtgPXDXzba7M/DDkddPBta12vehGz5kX7pvR19LN0bTDsBX6H6x7wR8Gdh7pPY1Y/p3AvCZtuwKurGfdpmvxra9n29e78j6Rvs832fx2Fbb3D149mzPpwMr2vSD6YY3mlvvq4CXT/vfoY+tfzhMi2bNTsA/Jrkf3VAid2/tZwNrkuwE/GtVnZvkUuBuSf4B+BTw2THrO5nuF/IZdMPtvLO13zvJ64Hd6YaoOe0W1HgocJ8kT2mv70j3S/e7I/PsBYyO/vsw4EPVHQq7MskXgAcCPwG+VlUbAZKcSxeyP6Yb9HJd23FZBsw3htNHqht195L2M7nnAjX+um3vu+NXdRPzfRaPAt5f7R48VXV1kjsAD+XGoX6gC7M5V7W6NOMMFW0rLqIbnHMxfwlcCdyX7q/vX0J3c6IkDwceRzcu25ur6qQk9wUeAxwN/CndeYxRa4H/mWRPuj2bz7X2E4AnVtV5SZ5D9xf85q7nxkPIu4y0h27PZqEg+sWYZeYzOir0DXT/bwNcVFUPWWC5OZuPxVTz1ZjkEXR7Kn2M/Szaujff5g7Aj6vqfvOsaxe6n4lmnOdUtK34HHCbJM+fa0jywCSb37vljsAV7S/vZ9P9hU6S3weuqqr30A34ef90I+3uUFUfB/6W7jarN1FVPwO+Brwd+GTdeNJ8V+CKtufzzHlqvowuiOCmgXga8IK2LOmuYLv9Ztu9BliWZC5YzgSe1s517E13W9ivzbNd6Ial3zvdPUBo54HuNc+8T02yQ5ID6G6te3GfGnsY+1nQ7RE+b+48VJI9q+onwHeTPLW1pQX+nLsD28QVfto6hoq2CVVVwJOAR6e7pPgiuvMXm9/r4Z3AqiRn0f0imvur+hHAuUm+QXd+4u10d7T7fDtkdALwynk2fzLwrPY852+Br9Kd5/j2PMv9Pd0v5i9z03twvBf4JvD1dJdC/xPjjwp8lu6wF3RXvp1Pdx/1zwGvqKofzrNdqrs97lOANyY5DziX7vDSOBcDX6C7Y+efV9Uvb0GNCxn7WVTVZ+j2ANe3n/3cJdjPBI5q9V7ETW/tezDwf27h9rUNcpRiaUrS3afmZVX17Alu4wS6PbCPTWobW2spfg5aOu6pSFNSVd8AzsgMfflxQvai2zPUrYB7KpKkwbinIkkajKEiSRqMoSJJGoyhIkkajKEiSRqMoSJJGsz/B2hTO5VVBjsTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print ('Number of unique classes:', len(df.default))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(df.default )\n",
    "plt.xlabel('Class value (one per face)')\n",
    "plt.ylabel('Class frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy, precision, recall?\n",
    "- How do you perform precision or recall when there are mutliple classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9a3a71e89085>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# get a handle to the classifier object, which defines the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# create variables we are more familiar with\n",
    "X = df[continuous_features]\n",
    "y = df.default\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "\n",
    "scl = StandardScaler()\n",
    "X = scl.fit_transform(X)\n",
    "\n",
    "# create cross validation iterator\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# get a handle to the classifier object, which defines the type\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Class value (one per face)')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "plot_class_acc(y,yhat,title=\"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't seem to be doing very well... Perhaps we should change the classifier we are using--why would this be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:841: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-af0bc74893bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# now iterate through and get predictions, saved to the correct row in yhat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=50, n_estimators=150, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf.fit(X[train],y[train])\n",
    "    yhat[test] = clf.predict(X[test])\n",
    "    \n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Accuracy', total_accuracy)\n",
    "plot_class_acc(y,yhat,title=\"Random Forest, Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets get access to the different properties of our RF\n",
    "\n",
    "print (clf)\n",
    "\n",
    "plt.barh(range(len(clf.feature_importances_)), clf.feature_importances_)\n",
    "plt.show()\n",
    "\n",
    "print ('Generalization score estimate from training data', clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 50\n",
    "w = 37\n",
    "plt.imshow(clf.feature_importances_.reshape((h, w)), cmap=plt.cm.gray)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Ensemble Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_estimators = 50\n",
    "# lets train some trees\n",
    "clf_array = [\n",
    "    ('Stump',              DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)),\n",
    "    ('Tree',               DecisionTreeClassifier()),\n",
    "    ('Random Trees',       RandomForestClassifier(max_depth=50, n_estimators=num_estimators)),\n",
    "    ('Extra Random Trees', ExtraTreesClassifier(n_estimators=num_estimators,min_samples_split=2)),\n",
    "    ('Boosted Tree',       GradientBoostingClassifier(n_estimators=num_estimators)), #takes a long time\n",
    "    ]\n",
    "\n",
    "for clf in clf_array:\n",
    "    acc = cross_val_score(clf[1],X,y)\n",
    "    print (clf[0], acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# setup pipeline to take PCA, then fit a different classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=100,svd_solver='randomized')),\n",
    "     ('CLF',GaussianNB())]\n",
    ")\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)\n",
    "plot_class_acc(y,yhat,title=\"Naive Bayes + PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "\n",
    "freq_infreq_threshold = 40\n",
    "\n",
    "# get various measures of performance\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "per_class_acc_list = per_class_accuracy(y,yhat)\n",
    "\n",
    "prec_for_freq_classes = []\n",
    "recall_for_infreq_classes = []\n",
    "rec_tot = []\n",
    "prec_tot = []\n",
    "\n",
    "for cls in np.unique(y):\n",
    "    idx = (y==cls) # get classes\n",
    "    ytmp_actual = np.zeros(y.shape) # make binary class problem\n",
    "    ytmp_actual[idx] = 1 # set the instances for this specific class\n",
    "    \n",
    "    ytmp_predicted = np.zeros(y.shape) # binary prediction array\n",
    "    ytmp_predicted[yhat==cls] = 1\n",
    "    \n",
    "    num_in_class = sum(idx)\n",
    "    \n",
    "    rec = mt.recall_score(ytmp_actual, ytmp_predicted)\n",
    "    prec = mt.precision_score(ytmp_actual, ytmp_predicted)\n",
    "    rec_tot.append(rec)\n",
    "    prec_tot.append(prec)\n",
    "    \n",
    "    if num_in_class < freq_infreq_threshold:\n",
    "        recall_for_infreq_classes.append(rec)\n",
    "    elif num_in_class >= freq_infreq_threshold:\n",
    "        prec_for_freq_classes.append(prec)\n",
    "        \n",
    "print ('Total Accuracy:',total_accuracy)\n",
    "print ('Number of infrequent faces:',len(recall_for_infreq_classes), \n",
    "       'with average recall of:', np.mean(recall_for_infreq_classes))\n",
    "print ('Number of frequent faces:',len(prec_for_freq_classes), \n",
    "       'with average precision of:',np.mean(prec_for_freq_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can really summarize this data much better than this. \n",
    "# How about looking at more statistics of the precision and recall for each class?\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.boxplot([ rec_tot, prec_tot, recall_for_infreq_classes,prec_for_freq_classes],\n",
    "            labels=['Recall, all faces','Precision, all faces','Recall Infreq. faces','Prec Freq. faces'])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about plotting a confusion matrix?\n",
    "cm = mt.confusion_matrix(y, yhat)\n",
    "plt.imshow(cm,cmap=plt.get_cmap('Reds'),aspect='auto')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the problem with this graph??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 32-34 statcompare.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# setup pipeline to take PCA, then fit a different classifier\n",
    "clf_pipe = Pipeline(\n",
    "    [('PCA',PCA(n_components=100,svd_solver='randomized')),\n",
    "     ('CLF',GaussianNB())]\n",
    ")\n",
    "\n",
    "yhat_score = np.zeros((y.shape[0],len(lfw_people.target_names)))\n",
    "\n",
    "# now iterate through and get predictions, saved to the correct row in yhat\n",
    "for train, test in cv.split(X,y):\n",
    "    clf_pipe.fit(X[train],y[train])\n",
    "    yhat[test] = clf_pipe.predict(X[test])\n",
    "    yhat_score[test] = clf_pipe.predict_proba(X[test])\n",
    "\n",
    "total_accuracy = mt.accuracy_score(y, yhat)\n",
    "print ('Pipeline accuracy', total_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ROC Curve\n",
    "In the below example we can see how to graph the false negatives and false positives in an ROC curve for a given classifier. Please note that the \"scores\" from the classifier have already been populated using the code above. The scores can be interpretted as the the probability that a given class should be designated as positive. These scores are needed so that the ROC can change thresholds deciding if theclass is positive or negative. Once we have the scores, we can send each column of scores (i.e., the probability for that class being positive) into the ROC curve generator and it will give use the arrays of false positives and negatives for that class as the threshold is increased. \n",
    "\n",
    "We save the outputs into a dictionary of fpr and tpr (false positive and true positive rates). The keys to the dictionary are the class value. We can also compute the ROC treshold for all the classes by placing their probabilities into one giant vector and whether they should or should not be a value of one. Please note that this method of combining all classes into a single ROC is not considered a standard method--it has limited utility outside classifier comparison. If you understand the limitations it can be an effective tool. If not, then I would not recommend using it to compare models right away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_palette(\"dark\")\n",
    "# code manipulated from http://scikit-learn.org/stable/auto_examples/plot_roc.html\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Compute ROC curve for a subset of interesting classes\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in np.unique(y):\n",
    "    fpr[i], tpr[i], _ = mt.roc_curve(y, yhat_score[:, i], pos_label=i)\n",
    "    roc_auc[i] = mt.auc(fpr[i], tpr[i])\n",
    "\n",
    "for i in np.random.permutation(60)[0:6]:\n",
    "    plt.plot(fpr[i], tpr[i], label='class {0} with {1} instances (area = {2:0.2f})'\n",
    "                                   ''.format(i, sum(y==i), roc_auc[i]))\n",
    "\n",
    "plt.legend(loc=\"lower right\")  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined ROC over all classes\n",
    "one_hot_class_encoding = label_binarize(y,np.unique(y))\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = mt.roc_curve(one_hot_class_encoding.ravel(), yhat_score.ravel())\n",
    "roc_auc[\"micro\"] = mt.auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Comparison \n",
    "- how might you compare two classifiers trained on the exact same datasets?\n",
    "\n",
    "Note: if these cells are not loaded, you might need to run the `%load ` magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-15 statcompare.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 19-28 statcompare.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
